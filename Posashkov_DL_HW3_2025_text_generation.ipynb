{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gg2e0GYiqPjk"
      },
      "source": [
        "# Homework 3 - Text generation with LSTM and Transformer networks\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_hn9dHcPKt4"
      },
      "source": [
        "## Installs the unidecode library and downloads the Shakespeare dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qpdcKW580xG",
        "outputId": "5c64d299-73ee-450a-d1c1-a0c897330811"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.4.0-py3-none-any.whl.metadata (13 kB)\n",
            "Downloading Unidecode-1.4.0-py3-none-any.whl (235 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/235.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.8/235.8 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.4.0\n",
            "--2025-05-09 00:28:09--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2025-05-09 00:28:09 (66.4 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install unidecode\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UG5SIDaHKXC"
      },
      "source": [
        "## LSTM implementation\n",
        "\n",
        "For this task you will implement the LSTM neural network architecture and train it on the task of character-level text generation. Implement a single layer LSTM and optionally extend your implementation to multiple layers to generate better results.\n",
        "\n",
        "Links:\n",
        "\n",
        "- https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html -- Lists the equations for each component of the LSTM cell.\n",
        "- http://colah.github.io/posts/2015-08-Understanding-LSTMs/ -- Intuitive explanation of LSTM\n",
        "## - http://karpathy.github.io/2015/05/21/rnn-effectiveness/ -- Explanation and uses of RNNs.\n",
        "\n",
        "\n",
        "Implement the initialization and the forward pass of a LSTMCell and use it as part of the LSTMSimple network class.\n",
        "\n",
        "The input of the LSTM network will be a sequence of characters, whereas the input of the LSTMCell will be a single input character (x), the output of the previous iteration (C) and the hidden state of the previous iteration (h). Iteratively process the entire input character sequence and calculate the loss based on the prediction at each time step.\n",
        "\n",
        "### Do NOT use the torch.nn.LSTM class.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lW_d5V-irJBj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "from torch.autograd import Variable\n",
        "\n",
        "class LSTMCell(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(LSTMCell, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "        # Input gate parameters\n",
        "        self.W_ii = nn.Parameter(torch.Tensor(hidden_dim, input_dim))\n",
        "        self.W_hi = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
        "        self.b_ii = nn.Parameter(torch.Tensor(hidden_dim))\n",
        "        self.b_hi = nn.Parameter(torch.Tensor(hidden_dim))\n",
        "\n",
        "        # Forget gate parameters\n",
        "        self.W_if = nn.Parameter(torch.Tensor(hidden_dim, input_dim))\n",
        "        self.W_hf = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
        "        self.b_if = nn.Parameter(torch.Tensor(hidden_dim))\n",
        "        self.b_hf = nn.Parameter(torch.Tensor(hidden_dim))\n",
        "\n",
        "        # Cell gate parameters\n",
        "        self.W_ig = nn.Parameter(torch.Tensor(hidden_dim, input_dim))\n",
        "        self.W_hg = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
        "        self.b_ig = nn.Parameter(torch.Tensor(hidden_dim))\n",
        "        self.b_hg = nn.Parameter(torch.Tensor(hidden_dim))\n",
        "\n",
        "        # Output gate parameters\n",
        "        self.W_io = nn.Parameter(torch.Tensor(hidden_dim, input_dim))\n",
        "        self.W_ho = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
        "        self.b_io = nn.Parameter(torch.Tensor(hidden_dim))\n",
        "        self.b_ho = nn.Parameter(torch.Tensor(hidden_dim))\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        \"\"\"Initialize weights according to the LSTM paper suggestion\"\"\"\n",
        "        for param in self.parameters():\n",
        "            if len(param.shape) >= 2:\n",
        "                nn.init.xavier_uniform_(param)\n",
        "            else:\n",
        "                nn.init.zeros_(param)\n",
        "\n",
        "        # Set forget gate bias to 1 as per best practices\n",
        "        nn.init.ones_(self.b_if)\n",
        "        nn.init.ones_(self.b_hf)\n",
        "\n",
        "    def forward(self, x, C, h):\n",
        "        \"\"\"\n",
        "        Forward pass of the LSTM cell\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch_size, input_dim)\n",
        "            C: Previous cell state of shape (batch_size, hidden_dim)\n",
        "            h: Previous hidden state of shape (batch_size, hidden_dim)\n",
        "\n",
        "        Returns:\n",
        "            C_out: Updated cell state\n",
        "            h_out: Updated hidden state\n",
        "        \"\"\"\n",
        "        # Input gate\n",
        "        i_t = torch.sigmoid(x @ self.W_ii.t() + self.b_ii + h @ self.W_hi.t() + self.b_hi)\n",
        "\n",
        "        # Forget gate\n",
        "        f_t = torch.sigmoid(x @ self.W_if.t() + self.b_if + h @ self.W_hf.t() + self.b_hf)\n",
        "\n",
        "        # Cell candidate\n",
        "        g_t = torch.tanh(x @ self.W_ig.t() + self.b_ig + h @ self.W_hg.t() + self.b_hg)\n",
        "\n",
        "        # Output gate\n",
        "        o_t = torch.sigmoid(x @ self.W_io.t() + self.b_io + h @ self.W_ho.t() + self.b_ho)\n",
        "\n",
        "        # Update cell state: c_t = f_t * c_{t-1} + i_t * g_t\n",
        "        C_out = f_t * C + i_t * g_t\n",
        "\n",
        "        # Update hidden state: h_t = o_t * tanh(c_t)\n",
        "        h_out = o_t * torch.tanh(C_out)\n",
        "\n",
        "        return C_out, h_out\n",
        "\n",
        "\n",
        "class LSTMSimple(nn.Module):\n",
        "    \"\"\"\n",
        "    Enhanced LSTM implementation with multi-layer support and dropout\n",
        "    with backward compatibility for sampling functions\n",
        "    \"\"\"\n",
        "    def __init__(self, seq_length, input_dim, hidden_dim, output_dim, batch_size, num_layers=1, dropout=0.0):\n",
        "        super(LSTMSimple, self).__init__()\n",
        "        self.seq_length = seq_length\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.batch_size = batch_size\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # Create LSTM cells for each layer\n",
        "        self.lstm_cells = nn.ModuleList()\n",
        "\n",
        "        # First layer takes input from the input dimension\n",
        "        self.lstm_cells.append(LSTMCell(input_dim, hidden_dim, output_dim))\n",
        "\n",
        "        # Subsequent layers take input from the hidden dimension of the previous layer\n",
        "        for _ in range(1, num_layers):\n",
        "            self.lstm_cells.append(LSTMCell(hidden_dim, hidden_dim, output_dim))\n",
        "\n",
        "        self.lstm_cell = self.lstm_cells[0]\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout_layer = nn.Dropout(dropout)\n",
        "\n",
        "        # Projection layer to output dimension\n",
        "        self.proj = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def init_hidden(self, batch_size, device=None):\n",
        "        \"\"\"Initialize hidden and cell states\"\"\"\n",
        "        if device is None:\n",
        "            device = next(self.parameters()).device\n",
        "\n",
        "        return (torch.zeros(batch_size, self.hidden_dim, device=device),\n",
        "                torch.zeros(batch_size, self.hidden_dim, device=device))\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass over a sequence of characters\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch_size, seq_length, input_dim)\n",
        "\n",
        "        Returns:\n",
        "            outputs: Predictions for each step in the sequence\n",
        "            (C, h): Final cell and hidden states of the last layer\n",
        "        \"\"\"\n",
        "        #  the case when x is just a sequence without batch dimension\n",
        "        if len(x.shape) == 2:\n",
        "            x = x.unsqueeze(0)  # batch dimension\n",
        "\n",
        "        batch_size, seq_length, _ = x.size()\n",
        "        device = x.device\n",
        "        cell_states = [torch.zeros(batch_size, self.hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "        hidden_states = [torch.zeros(batch_size, self.hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "\n",
        "        outputs = []\n",
        "        for t in range(seq_length):\n",
        "            x_t = x[:, t, :]\n",
        "\n",
        "            # Process through each layer\n",
        "            for layer in range(self.num_layers):\n",
        "                if layer > 0:\n",
        "                    # Apply dropout between layers\n",
        "                    x_t = self.dropout_layer(x_t)\n",
        "                # Forward through LSTM cell\n",
        "                cell_states[layer], hidden_states[layer] = self.lstm_cells[layer](\n",
        "                    x_t, cell_states[layer], hidden_states[layer]\n",
        "                )\n",
        "                x_t = hidden_states[layer]\n",
        "\n",
        "            # Apply dropout before projection layer\n",
        "            final_hidden = self.dropout_layer(hidden_states[-1])\n",
        "\n",
        "            # Project to output dimension\n",
        "            out = self.proj(final_hidden)\n",
        "            outputs.append(out)\n",
        "\n",
        "\n",
        "        outputs = torch.stack(outputs, dim=1)  # Shape: (batch_size, seq_length, output_dim)\n",
        "\n",
        "\n",
        "        return outputs, (cell_states[-1], hidden_states[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3A1_UtJrKnU-"
      },
      "source": [
        "### LSTM Sampling Code\n",
        "\n",
        "To generate text the network must predict the next character in a sequence, however networks do not produce a single character but rather estimate the likelihood for each possible character. Sampling characters from the network output can be done in different ways with common ones being the Greedy sampling process and Top-K sampling.\n",
        "\n",
        "In the simple greedy sampling method the network takes a text prompt as input and generates an additional N tokens by always taking the token with the highest prediction score as the next token.\n",
        "\n",
        "In the Top-K sampling, randomness is added to the sampling process as the network samples from K most likely predicitons at each step. This alleviates the problem of generative models repeating text but may generate incorrect text by sampling inappropriate tokens.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "X23_lg53Kqj2"
      },
      "outputs": [],
      "source": [
        "def greedy_sampling_lstm(lstm, x, num_chars):\n",
        "    # x -- b x onehot_char\n",
        "    outputs = torch.zeros((1,num_chars,x.shape[2]))\n",
        "    t_outputs, (cell_state, hidden) = lstm(x.float())\n",
        "    for c in range(num_chars):\n",
        "        output_tmp = torch.softmax(lstm.proj(hidden),dim=1)\n",
        "        top_ind = torch.argmax(output_tmp,dim=1)[0]\n",
        "        tmp = torch.zeros_like(x[:,0,:]).cuda()\n",
        "        tmp[:,top_ind] = 1\n",
        "        outputs[:,c] = tmp\n",
        "\n",
        "        cell_state, hidden = lstm.lstm_cell(tmp,cell_state,hidden)\n",
        "    return outputs\n",
        "\n",
        "def topk_sampling_lstm(lstm, x, num_chars):\n",
        "    # x -- b x onehot_char\n",
        "    outputs = torch.zeros((1,num_chars,x.shape[2]))\n",
        "    t_outputs, (cell_state, hidden) = lstm(x.float())\n",
        "    for c in range(num_chars):\n",
        "        output_vals, output_ind = torch.topk(lstm.proj(hidden), 5, dim=1)\n",
        "        output_tmp = torch.softmax(output_vals,dim=1)\n",
        "        top_ind = torch.multinomial(output_tmp[0], 1)[0]\n",
        "        tmp = torch.zeros_like(x[:,0,:]).cuda()\n",
        "        tmp[:,output_ind[0,top_ind]] = 1\n",
        "        outputs[:,c] = tmp\n",
        "\n",
        "        cell_state, hidden = lstm.lstm_cell(tmp,cell_state,hidden)\n",
        "\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJdxAOVsKzBX"
      },
      "source": [
        "### LSTM Dataset Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "G5U4jzUDK1dG"
      },
      "outputs": [],
      "source": [
        "import unidecode\n",
        "import string\n",
        "import random\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class LSTMDataset(Dataset):\n",
        "    def __init__(self, chunk_len=200, padded_chunks=False):\n",
        "        # Character based dataset\n",
        "        dataset_path = \"./input.txt\"\n",
        "        # The tokens in the vocabulary (all_characters)\n",
        "        # are just the printable characters of the string class\n",
        "        self.all_characters = string.printable\n",
        "        self.n_characters = len(self.all_characters)\n",
        "        # Maps characters to indices\n",
        "        self.char_dict = {x:i for i,x in enumerate(self.all_characters)}\n",
        "        self.file, self.file_len = self.read_file(dataset_path)\n",
        "        # Sequence length of the input\n",
        "        self.chunk_len = chunk_len\n",
        "\n",
        "    def read_file(self,filename):\n",
        "        file = unidecode.unidecode(open(filename).read())\n",
        "        return file, len(file)\n",
        "\n",
        "    def char_tensor(self,in_str):\n",
        "        # in_str - input sequence - String\n",
        "        # Return one-hot encoded characters of in_str\n",
        "        tensor = torch.zeros(len(in_str),self.n_characters).long()\n",
        "        char_ind = [self.char_dict[c] for c in in_str]\n",
        "        tensor[torch.arange(tensor.shape[0]),char_ind] = 1\n",
        "        return tensor\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inp, target = self.get_random_text()\n",
        "        return {\"input\":inp, \"target\":target}\n",
        "\n",
        "    def __len__(self):\n",
        "        return 10000\n",
        "\n",
        "    def get_random_text(self):\n",
        "        # Pick a random string of length self.chunk_len from the dataset\n",
        "        start_index = np.random.randint(0, self.file_len - self.chunk_len)\n",
        "        end_index = start_index + self.chunk_len + 1\n",
        "        chunk = self.file[start_index:end_index]\n",
        "        # One-hot encode the chosen string\n",
        "        inp = self.char_tensor(chunk[:-1])\n",
        "        # The target string is the same as the\n",
        "        # input string but shifted by 1 character\n",
        "        target = self.char_tensor(chunk[1:])\n",
        "        inp = Variable(inp).cuda()\n",
        "        target = Variable(target).cuda()\n",
        "        return inp, target\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5S7JDlDZRqrg"
      },
      "source": [
        "### LSTM Training loop\n",
        "\n",
        "With a correct implementation you should get sensible text generation results with the set parameters, however you should experiment with various parameters,\n",
        "especially with the sequence length (chunk_len) used during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDVof1Qe1_vG",
        "outputId": "33940c21-7db6-4aa3-9d00-5c975a60f1d3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 0/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 863.01chunks/s, loss=3.31, lr=0.005, run:=LSTM]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling -----------------\n",
            "O Romeo, wherefore art thou e eeean     oote ete tethe  toe  o e  a e  ae a  e   eotteea e   attoeat   teeteeotoetoa eoeoet to tet  teeeee o  o ano t the  t   e e tt t to ate   eo  te t  te  aon  e  tha eoneea ae   athto etthao ao  otoao at e e oth ao  a et a  t    teaone  tt  o t o  to   t et to ttteee   oette tet ano tteea \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTraining - Epoch: 0/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 772.32chunks/s, loss=3.31, lr=0.005, run:=LSTM]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy sampling ----------------\n",
            "O Romeo, wherefore art thou                                                                                                                                                                                                                                                                                                            \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 1/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 848.98chunks/s, loss=2.92, lr=0.005, run:=LSTM]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling -----------------\n",
            "O Romeo, wherefore art thoun anesan s  one t te thethote sh  otesas  ha e nes ta  oe t s na    tandth   earant  tee o   othin athenoses athe  at e  hiso t   eo ar so to oto sa oot t tare te so atho tase t e  hi ter  hitet tothint ate norit a to  ae se  o e  aeere th  ond s thate andete ot a thetoeto ooeetet sho an t e  he  t \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTraining - Epoch: 1/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 783.35chunks/s, loss=2.92, lr=0.005, run:=LSTM]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy sampling ----------------\n",
            "O Romeo, wherefore art thou  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 2/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 838.40chunks/s, loss=2.51, lr=0.005, run:=LSTM]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling -----------------\n",
            "O Romeo, wherefore art thou  int, the me t the sous wou ther he s wees hor, hor ter thiret onthan hile wo the will seer har to sin wont aote wor seser hin we mire sing and ar mes the s ou hire tord we sous tord thon thite mas and tour sor se mer ther whathe he sot, what, the th mat he tot and wourd an sesthe sors thes,\n",
            "Thent \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTraining - Epoch: 2/30: 100%|█████████▉| 9984/10000 [00:13<00:00, 766.39chunks/s, loss=2.51, lr=0.005, run:=LSTM]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy sampling ----------------\n",
            "O Romeo, wherefore art thou the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 3/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 699.00chunks/s, loss=2.38, lr=0.005, run:=LSTM]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling -----------------\n",
            "O Romeo, wherefore art thou hease thithat to then sind.\n",
            "\n",
            "LIRENO:IT:\n",
            "Witheethat sors ar therstisthat hale hine seringof mas sendee hishes, wimis art heass, wat merthe the sorerend,\n",
            "\n",
            "Ar me than mirind ande hind my ardallis on win hars oothe meald the tharse sithind toul shathe thals, he there weren thint,\n",
            "Buthale we tort alese \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTraining - Epoch: 3/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 769.51chunks/s, loss=2.38, lr=0.005, run:=LSTM]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy sampling ----------------\n",
            "O Romeo, wherefore art thou the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 4/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 647.85chunks/s, loss=2.29, lr=0.005, run:=LSTM]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling -----------------\n",
            "O Romeo, wherefore art thou hang tous so the so te sord mesest tor hart ond he my therene houn me tind to ther, shour the hime, tort stise thald stars stend,\n",
            "As tathis shim art ofered\n",
            "\n",
            "Bo hertis all seres,\n",
            "Ande the ho chen an tend ther then, tour sous ous some,\n",
            "Weld that an blowing as sesest,'dis so me tat mes ar meng ouss st\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTraining - Epoch: 4/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 772.18chunks/s, loss=2.29, lr=0.005, run:=LSTM]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy sampling ----------------\n",
            "O Romeo, wherefore art thou the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 5/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 703.07chunks/s, loss=2.2, lr=0.005, run:=LSTM]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling -----------------\n",
            "O Romeo, wherefore art thour wit homate st ard ould aster hared tould wars on all me tound, busther well thall wot and hist, an sing his tou hat, the with the winthe, and hare hithare sont thit hat the thot he wo d at the my ur mond then woll an thee thath and to that heer will ther whel ter will te the hert and we him toure \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTraining - Epoch: 5/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 787.98chunks/s, loss=2.2, lr=0.005, run:=LSTM]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy sampling ----------------\n",
            "O Romeo, wherefore art thou the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 6/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 778.32chunks/s, loss=2.14, lr=0.005, run:=LSTM]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling -----------------\n",
            "O Romeo, wherefore art thoul sises, wothe the ward and astof are ast hears at mas mand,\n",
            "Whe the somenes at tith thim sind the sent thil merthy,\n",
            "And way the wit sithes at ard to ther some, are sis in the woredsens an mo she the hard,\n",
            "And will with the stee heard as all all the hing that thas ath sings an the priss,\n",
            "And homy th\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTraining - Epoch: 6/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 787.45chunks/s, loss=2.14, lr=0.005, run:=LSTM]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy sampling ----------------\n",
            "O Romeo, wherefore art thou the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 7/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 829.62chunks/s, loss=2.07, lr=0.005, run:=LSTM]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling -----------------\n",
            "O Romeo, wherefore art thou wis and be ancaless of of all we word hom mand\n",
            "Thou her to to must and wouther my shath my tored, thee time toom thou here thees,\n",
            "And to meart and, the preang the hond thene thould the too for stouns in thinge tis me thou deast as me sheer and wet to thas ar he woll wele, and bu to tood with ther, \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTraining - Epoch: 7/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 784.92chunks/s, loss=2.07, lr=0.005, run:=LSTM]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy sampling ----------------\n",
            "O Romeo, wherefore art thou have the the the the the the will the the will the the will the the will the the will the the will the the will the the will the the will the the will the the will the the will the the will the the will the the will the the will the the will the the will the the will the the will the the will the t\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 8/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 862.10chunks/s, loss=2.02, lr=0.005, run:=LSTM]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling -----------------\n",
            "O Romeo, wherefore art thourd thath we my a pould the tard ald in still and an and,\n",
            "And with and wely deat it in migh tis thy have sould that we he the pritith.\n",
            "\n",
            "CEONIENE:\n",
            "A wir heave that a deer hinst, as, at in the hande ant head.\n",
            "The shear this and withee an al is mard.\n",
            "\n",
            "CLOUCIS:\n",
            "And we has and his sand hored wires on that\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTraining - Epoch: 8/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 781.48chunks/s, loss=2.02, lr=0.005, run:=LSTM]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy sampling ----------------\n",
            "O Romeo, wherefore art thou have the will the will the will the will the will the will the will the will the will the will the will the will the will the will the will the will the will the will the will the will the will the will the will the will the will the will the will the will the will the will the will the will the wi\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 9/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 846.15chunks/s, loss=1.96, lr=0.005, run:=LSTM]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling -----------------\n",
            "O Romeo, wherefore art thou so mese, the wetring the wall this the will shimed are to moust,\n",
            "The shell striss, and the to that how, the she soule and, thingenter, the singhes.\n",
            "\n",
            "CANIONT:\n",
            "No ther seed you mane sire mose the peres ath hear\n",
            "\n",
            "Pringe tis send, with sonds or shatl here fart him.\n",
            "\n",
            "POLIO:\n",
            "No these then will he to more\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTraining - Epoch: 9/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 779.30chunks/s, loss=1.96, lr=0.005, run:=LSTM]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy sampling ----------------\n",
            "O Romeo, wherefore art thou have the seare the will the seare the will the seare the will the seare the will the seare the will the seare the will the seare the will the seare the will the seare the will the seare the will the seare the will the seare the will the seare the will the seare the will the seare the will the seare\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 10/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 867.06chunks/s, loss=1.93, lr=0.005, run:=LSTM]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling -----------------\n",
            "O Romeo, wherefore art thou hownes of has in\n",
            "The tay torstence, the wondse this spurtise,\n",
            "Thar seat that soures on my, whencence the sorser, ang\n",
            "To son anglededse the sang of tor his and and hither shill and the painged,\n",
            "Anderst to me here folly\n",
            "And the bade ofthare for mear of to mar to hish strepence to mentes,\n",
            "And that hav\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTraining - Epoch: 10/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 783.28chunks/s, loss=1.93, lr=0.005, run:=LSTM]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy sampling ----------------\n",
            "O Romeo, wherefore art thou have the seare the seare the seare the seare the seare the seare the seare the seare the seare the seare the seare the seare the seare the seare the seare the seare the seare the seare the seare the seare the seare the seare the seare the seare the seare the seare the seare the seare the seare the \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 11/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 866.39chunks/s, loss=1.88, lr=0.005, run:=LSTM]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling -----------------\n",
            "O Romeo, wherefore art thou down.\n",
            "\n",
            "CLADINA:\n",
            "Then mer is the ceane tree, to me that home.\n",
            "So llot of the cour well me,\n",
            "I will wet that well that has and, be the sone thy lare\n",
            "To true shat to makes my ase,\n",
            "The hought hear sind on than ale she to think\n",
            "Than to hat this ald that have the perines.\n",
            "That the diend mentor to my love \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTraining - Epoch: 11/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 780.91chunks/s, loss=1.88, lr=0.005, run:=LSTM]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy sampling ----------------\n",
            "O Romeo, wherefore art thou have the sear the sond the sear the sond the sear the sond the sear the sond the sear the sond the sear the sond the sear the sond the sear the sond the sear the sond the sear the sond the sear the sond the sear the sond the sear the sond the sear the sond the sear the sond the sear the sond the se\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 12/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 867.64chunks/s, loss=1.83, lr=0.005, run:=LSTM]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling -----------------\n",
            "O Romeo, wherefore art thou the ploves\n",
            "Will wotld to be shall were wits of the sind\n",
            "And will hisser to thee the sones\n",
            "This faith thou brient to but merelower his thee,\n",
            "Take a dond you thes and tho grom of the pray he hiss.\n",
            "\n",
            "CARIOLANUS:\n",
            "How hishire, sor, whe she word hath remence te hild as\n",
            "To so to befers mis as to the sont\n",
            "T\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTraining - Epoch: 12/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 787.19chunks/s, loss=1.83, lr=0.005, run:=LSTM]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy sampling ----------------\n",
            "O Romeo, wherefore art thou have the seart the seart the seart the seart the seart the seart the seart the seart the seart the seart the seart the seart the seart the seart the seart the seart the seart the seart the seart the seart the seart the seart the seart the seart the seart the seart the seart the seart the seart the \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 13/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 861.60chunks/s, loss=1.82, lr=0.005, run:=LSTM]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling -----------------\n",
            "O Romeo, wherefore art thou to murter.\n",
            "\n",
            "PEONTILANG:\n",
            "With he sin than the should hase but tays as he heather than sin\n",
            "To than anderst onterthe all woon thou and and tome, shee, he wasch that wish shere,\n",
            "Thou down thoush treep it treather to menchare still be than thy horess.\n",
            "\n",
            "KING RICHARD III:\n",
            "Whonese, his fither the see then \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTraining - Epoch: 13/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 780.77chunks/s, loss=1.82, lr=0.005, run:=LSTM]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy sampling ----------------\n",
            "O Romeo, wherefore art thou have the seart the seart the seart the seart the seart the seart the seart the seart the seart the seart the seart the seart the seart the seart the seart the seart the seart the seart the seart the seart the seart the seart the seart the seart the seart the seart the seart the seart the seart the \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 14/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 862.86chunks/s, loss=1.78, lr=0.005, run:=LSTM]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling -----------------\n",
            "O Romeo, wherefore art thought.\n",
            "\n",
            "KING HENRY BI I'll near his treeder, and hup tane.\n",
            "\n",
            "KING RICHARD III\n",
            "ABELRETHE:\n",
            "The with that the seest the dondes ar she, all to have have home,\n",
            "And hingst be men tring triess the courte\n",
            "To have to the prater thank'd that well ane ward owe,\n",
            "Bur I ward hill to me prose all to tearth me.\n",
            "\n",
            "Forst\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTraining - Epoch: 14/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 785.27chunks/s, loss=1.78, lr=0.005, run:=LSTM]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy sampling ----------------\n",
            "O Romeo, wherefore art thou have the proper the prace the prace the prace the prace the prace the prace the prace the prace the prace the prace the prace the prace the prace the prace the prace the prace the prace the prace the prace the prace the prace the prace the prace the prace the prace the prace the prace the prace the\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 15/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 869.87chunks/s, loss=1.76, lr=0.005, run:=LSTM]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling -----------------\n",
            "O Romeo, wherefore art thou houses and that to sie,\n",
            "Thou well but thim hers of you her with our santer\n",
            "And seed thee indery sirest hather,\n",
            "As the cames it,--\n",
            "The histring, but to may you with her seed his a been.\n",
            "\n",
            "PEROMASPARD:\n",
            "Well'd me the coust of haster are,\n",
            "Should be to that's telpis all hath sontersed held here\n",
            "I thee so\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTraining - Epoch: 15/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 787.71chunks/s, loss=1.76, lr=0.005, run:=LSTM]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy sampling ----------------\n",
            "O Romeo, wherefore art thou have the seed to the seed\n",
            "The see the see the see the seep the seep to the seed\n",
            "The see the see the see the seep the seep to the seed\n",
            "The see the see the see the seep the seep to the seed\n",
            "The see the see the see the seep the seep to the seed\n",
            "The see the see the see the seep the seep to the seed\n",
            "The\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 16/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 862.15chunks/s, loss=1.73, lr=0.005, run:=LSTM]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling -----------------\n",
            "O Romeo, wherefore art thou will.\n",
            "\n",
            "CORIOLANUS:\n",
            "And thiness the will with himes.\n",
            "\n",
            "LUCETIUS:\n",
            "I'll will his shall to the bank.\n",
            "\n",
            "LEONTES:\n",
            "Thise any thence my like of hatter and and all which shall be some,\n",
            "In would that wored and to my breatiness,\n",
            "I seate my sourthen, and thought on thee trust of\n",
            "Wordes to the woon some one the p\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTraining - Epoch: 16/30: 100%|█████████▉| 9984/10000 [00:13<00:00, 764.49chunks/s, loss=1.73, lr=0.005, run:=LSTM]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy sampling ----------------\n",
            "O Romeo, wherefore art thou have the words and the seed\n",
            "The seep the seep the seeple the seed the words and the seed\n",
            "The seep the seep the seeple the seed the words and the seed\n",
            "The seep the seep the seeple the seed the words and the seed\n",
            "The seep the seep the seeple the seed the words and the seed\n",
            "The seep the seep the seepl\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 17/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 854.12chunks/s, loss=1.67, lr=0.005, run:=LSTM]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling -----------------\n",
            "O Romeo, wherefore art thou am shall be no lore\n",
            "Toll men thou his angurent thy grouth,\n",
            "I' will be she should book nowere and more ant would\n",
            "Why, him, I well this wards of the caured.\n",
            "\n",
            "COMINIUS:\n",
            "And the king thought of you the wenk again,\n",
            "As a gont my lers of your hight we home all me\n",
            "Angullen spaticess we would that have trea\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTraining - Epoch: 17/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 784.70chunks/s, loss=1.67, lr=0.005, run:=LSTM]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy sampling ----------------\n",
            "O Romeo, wherefore art thou have the proper.\n",
            "\n",
            "PETRUCHIO:\n",
            "What shall be the propes the propess of the come to the proper.\n",
            "\n",
            "PETRUCHIO:\n",
            "What shall be the propes the propess of the come to the proper.\n",
            "\n",
            "PETRUCHIO:\n",
            "What shall be the propes the propess of the come to the proper.\n",
            "\n",
            "PETRUCHIO:\n",
            "What shall be the propes the propess of th\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 18/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 874.92chunks/s, loss=1.71, lr=0.005, run:=LSTM]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling -----------------\n",
            "O Romeo, wherefore art thou, neting ofe\n",
            "As ind you so day to the cair be the brest\n",
            "The partors and he be sorrow me. I keep in heard,\n",
            "And, brink and sould be tell me sheep,\n",
            "Tikn you sen thy can be te poos, and have mint.\n",
            "\n",
            "LUCIO:\n",
            "I thoughers there sere.\n",
            "\n",
            "PRITUS:\n",
            "All my sonter, shall,\n",
            "And have shall take a tade there and wile he\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTraining - Epoch: 18/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 776.76chunks/s, loss=1.71, lr=0.005, run:=LSTM]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy sampling ----------------\n",
            "O Romeo, wherefore art thou are the proper.\n",
            "\n",
            "KING RICHARD II:\n",
            "I will be the propess of the come to the proper.\n",
            "\n",
            "KING RICHARD II:\n",
            "I will be the propess of the come to the proper.\n",
            "\n",
            "KING RICHARD II:\n",
            "I will be the propess of the come to the proper.\n",
            "\n",
            "KING RICHARD II:\n",
            "I will be the propess of the come to the proper.\n",
            "\n",
            "KING RICHARD I\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 19/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 721.50chunks/s, loss=1.67, lr=0.005, run:=LSTM]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling -----------------\n",
            "O Romeo, wherefore art thou her shreads.\n",
            "\n",
            "GLOUCESTER:\n",
            "I'll say him be spork to thim have asment on the prove\n",
            "The stay daughty heavo stay strinker's my fisters,\n",
            "And beais down by mears of thee his has stoon spirt thee there to make\n",
            "What he word think he stown to the wingent me and wisted trat\n",
            "Whither stard'd my sourther midder\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTraining - Epoch: 19/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 776.09chunks/s, loss=1.67, lr=0.005, run:=LSTM]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy sampling ----------------\n",
            "O Romeo, wherefore art thou had the dears and the seed\n",
            "The see the seep the seep the seep the seep the seep the seep\n",
            "The see the seep the seep the seep the seep the seep the seep\n",
            "The see the seep the seep the seep the seep the seep the seep\n",
            "The see the seep the seep the seep the seep the seep the seep\n",
            "The see the seep the see\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 20/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 641.32chunks/s, loss=1.66, lr=0.005, run:=LSTM]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling -----------------\n",
            "O Romeo, wherefore art thou have not so to the see.\n",
            "\n",
            "LADY CAMPLLO:\n",
            "Andest whise serve the donger treast to mie.\n",
            "We hand son, see the dis a tongue.\n",
            "\n",
            "DUKE LARDARET:\n",
            "Well there a mest brother, but mastins warmes\n",
            "To blooven,--\n",
            "\n",
            "COMINIUS:\n",
            "We have myself, my lord out a thang,\n",
            "The mestisser,\n",
            "And the seep the disprysents to her, son \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTraining - Epoch: 20/30: 100%|█████████▉| 9984/10000 [00:13<00:00, 765.08chunks/s, loss=1.66, lr=0.005, run:=LSTM]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy sampling ----------------\n",
            "O Romeo, wherefore art thou art the propess.\n",
            "\n",
            "PETRUCHIO:\n",
            "What shall be the propess the see the propess of the propess.\n",
            "\n",
            "PETRUCHIO:\n",
            "What shall be the propess the see the propess of the propess.\n",
            "\n",
            "PETRUCHIO:\n",
            "What shall be the propess the see the propess of the propess.\n",
            "\n",
            "PETRUCHIO:\n",
            "What shall be the propess the see the propess of\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 21/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 714.61chunks/s, loss=1.63, lr=0.005, run:=LSTM]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling -----------------\n",
            "O Romeo, wherefore art thou and by my his oft.\n",
            "\n",
            "Stcond Grume:\n",
            "I they may be nutt them with thou als, to there tean\n",
            "Inderse the wreath the sunce mad by his come.\n",
            "\n",
            "CLOFFORY:\n",
            "Offul thee thee senst, and worder thee to has our\n",
            "There sor of heaven sir, and then them.\n",
            "\n",
            "LEONTES:\n",
            "It me treatine, a swill home so tone, with mister.\n",
            "\n",
            "CAT\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTraining - Epoch: 21/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 779.63chunks/s, loss=1.63, lr=0.005, run:=LSTM]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy sampling ----------------\n",
            "O Romeo, wherefore art thou art the seep.\n",
            "\n",
            "CORIOLANUS:\n",
            "What shall be so the comes to the seep to the seep\n",
            "The see the see the seep the seep to the seep\n",
            "The see the see the seep the seep to the seep\n",
            "The see the see the seep the seep to the seep\n",
            "The see the see the seep the seep to the seep\n",
            "The see the see the seep the seep to \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 22/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 780.73chunks/s, loss=1.6, lr=0.005, run:=LSTM]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling -----------------\n",
            "O Romeo, wherefore art thou and to\n",
            "Frink to maticumer age,\n",
            "The proceming, tell take held menty hath mighter, sording these a pricins,\n",
            "And than how when sord make a san ant ang storn\n",
            "Had say shall be the farmit and tongut,\n",
            "When, thereforames,'d a soon me tell her shome and him.\n",
            "\n",
            "LADY CAPULO:\n",
            "When I shoulds boon my stiends,\n",
            "Tha\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTraining - Epoch: 22/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 779.83chunks/s, loss=1.6, lr=0.005, run:=LSTM]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy sampling ----------------\n",
            "O Romeo, wherefore art thou art the words and the world\n",
            "That shall be the words and the words and the world the propent to the propent\n",
            "That the words and the propess to the propent the world\n",
            "That shall be the words and the words and the world the propent to the propent\n",
            "That the words and the propess to the propent the world\n",
            "T\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 23/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 832.25chunks/s, loss=1.59, lr=0.005, run:=LSTM]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling -----------------\n",
            "O Romeo, wherefore art thousat to my lead.\n",
            "\n",
            "PETRUCHIO:\n",
            "I warting hath moresess an housand tay.\n",
            "\n",
            "GREMIO:\n",
            "Toull, as I would this to the proved of hands in him;\n",
            "For what shall no barked thing out becime, here send him. \n",
            "First Mindrarman:\n",
            "I do may yell and book, a poor heart\n",
            "I'll but helloused him her shall not worthy.'\n",
            "Let of hu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTraining - Epoch: 23/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 777.81chunks/s, loss=1.59, lr=0.005, run:=LSTM]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy sampling ----------------\n",
            "O Romeo, wherefore art thou art the seep the seep\n",
            "That the see the see the seep the seep the seep\n",
            "That the see the see the seep the seep the seep\n",
            "That the see the see the seep the seep the seep\n",
            "That the see the see the seep the seep the seep\n",
            "That the see the see the seep the seep the seep\n",
            "That the see the see the seep the see\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 24/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 855.37chunks/s, loss=1.59, lr=0.005, run:=LSTM]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling -----------------\n",
            "O Romeo, wherefore art thou art as as your comple.\n",
            "\n",
            "LEONTES:\n",
            "That I were think, therefort, then shears\n",
            "And take itsears to heaving it alle.\n",
            "\n",
            "BIANCA:\n",
            "Is a head, this well that stay world,\n",
            "What well man your hims, there antagent,\n",
            "And blood wish and thou the stranger'd to this well:\n",
            "The crust the plantes of the clays.\n",
            "\n",
            "PORETER:\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTraining - Epoch: 24/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 784.22chunks/s, loss=1.59, lr=0.005, run:=LSTM]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy sampling ----------------\n",
            "O Romeo, wherefore art thou art the seep\n",
            "The stand the seep the seep the seep\n",
            "The stand the seep the seep the seep the seep\n",
            "The stand the seep the seep the seep the seep\n",
            "The stand the seep the seep the seep the seep\n",
            "The stand the seep the seep the seep the seep\n",
            "The stand the seep the seep the seep the seep\n",
            "The stand the seep \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 25/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 860.04chunks/s, loss=1.58, lr=0.005, run:=LSTM]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling -----------------\n",
            "O Romeo, wherefore art thou way;\n",
            "What, by my himes, and to thy ground.\n",
            "\n",
            "KING RENVE:\n",
            "How to the world as a good mady on and me.\n",
            "\n",
            "GRUMIO:\n",
            "We canned to mere it.\n",
            "\n",
            "LUCESTERS:\n",
            "I wad mad my seevy mean again,\n",
            "Thou dithar alos, to see how stride,\n",
            "We cay to see take, there hold, which thou hath to myseed thou so.\n",
            "\n",
            "Second Soran:\n",
            "No, sen\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTraining - Epoch: 25/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 782.01chunks/s, loss=1.58, lr=0.005, run:=LSTM]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy sampling ----------------\n",
            "O Romeo, wherefore art thou art the world\n",
            "That the world to the propece to the comes and the world\n",
            "That the world to the seep the seep to the comes and the world\n",
            "That the world to the seep the seep to the comes and the world\n",
            "That the world to the seep the seep to the comes and the world\n",
            "That the world to the seep the seep to \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 26/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 865.64chunks/s, loss=1.57, lr=0.005, run:=LSTM]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling -----------------\n",
            "O Romeo, wherefore art thou, meass trow the prisce,\n",
            "But the sundes of their hand out thy gaint\n",
            "He have stare of thee so do to she him shope her hold him his him.\n",
            "\n",
            "Second Gentleman:\n",
            "The coulds and the chood so deep diend which his child.\n",
            "\n",
            "KING RICHARD II:\n",
            "My fately shalt nave their childs. But you she him her heavy are\n",
            "I any b\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTraining - Epoch: 26/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 787.16chunks/s, loss=1.57, lr=0.005, run:=LSTM]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy sampling ----------------\n",
            "O Romeo, wherefore art thou art the seep.\n",
            "\n",
            "BAPTISTA:\n",
            "I shall be see the see the see the seep to the consent\n",
            "That the sent the sent the sent the sent the seep\n",
            "The see the see the see the seep the seep to the sent\n",
            "The see the see the see the seep the seep to the sent\n",
            "The see the see the see the seep the seep to the sent\n",
            "The see\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 27/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 853.12chunks/s, loss=1.57, lr=0.005, run:=LSTM]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling -----------------\n",
            "O Romeo, wherefore art thou art this that\n",
            "And house the brament, the heart.\n",
            "\n",
            "GRUMIO:\n",
            "Thy shall be nigners truch and sen thy brance\n",
            "I she had say him helfore as as you hate\n",
            "And be so forth and so shourderned wife,\n",
            "When to hear my look to be must stroud my strrie:\n",
            "How now stranch, I was songer we wart my blood.\n",
            "\n",
            "BRUTUS:\n",
            "If you,\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTraining - Epoch: 27/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 779.33chunks/s, loss=1.57, lr=0.005, run:=LSTM]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy sampling ----------------\n",
            "O Romeo, wherefore art thou art the see\n",
            "The see the see the seep the seep the seep\n",
            "That the world the see the seep the seep the seep\n",
            "That the world the see the seep the seep the seep\n",
            "That the world the see the seep the seep the seep\n",
            "That the world the see the seep the seep the seep\n",
            "That the world the see the seep the seep the\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 28/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 863.71chunks/s, loss=1.56, lr=0.005, run:=LSTM]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling -----------------\n",
            "O Romeo, wherefore art thou\n",
            "have book to too have her word.\n",
            "\n",
            "PATIS:\n",
            "Wardwnect, will no see thou dot here,\n",
            "And he hath now some an ale, with me.\n",
            "Why, and with your son of thou hadd sporl,\n",
            "Thou sailing to my father's sourt then with the world tell a port,\n",
            "And we well and to all thy best and the seared\n",
            "That so my his pray round \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTraining - Epoch: 28/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 780.03chunks/s, loss=1.56, lr=0.005, run:=LSTM]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy sampling ----------------\n",
            "O Romeo, wherefore art thou art the seep\n",
            "That the see the see the death and the see the death.\n",
            "\n",
            "KING RICHARD III:\n",
            "What is the see the see the see the death,\n",
            "And then the seep the seep the seep to the comes\n",
            "And the see the see the death and the seep\n",
            "That the see the see the death and the see the death.\n",
            "\n",
            "KING RICHARD III:\n",
            "What \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 29/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 870.95chunks/s, loss=1.53, lr=0.005, run:=LSTM]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling -----------------\n",
            "O Romeo, wherefore art thoughts,\n",
            "Which he be need to hush ther, be all,\n",
            "This beard he is arm all our here,\n",
            "It will be the wither't the confull a man the said to\n",
            "And, and his myself, thou wilt blain of their stainty\n",
            "Herry shoush a subled the hard on a comest alain,\n",
            "And him is to the can foon of yours,\n",
            "Sit wouch once hath them \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTraining - Epoch: 29/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 773.94chunks/s, loss=1.53, lr=0.005, run:=LSTM]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy sampling ----------------\n",
            "O Romeo, wherefore art thou hast here.\n",
            "\n",
            "CORIOLANUS:\n",
            "The hath the comes the streather than the seep\n",
            "That the strest the strest the streather than the seep\n",
            "That the strest the strest the streather than the seep\n",
            "That the strest the strest the streather than the seep\n",
            "That the strest the strest the streather than the seep\n",
            "That the\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "\n",
        "batch_size = 256\n",
        "chunk_len = 128\n",
        "model_name = \"LSTM\"\n",
        "train_dataset = LSTMDataset(chunk_len=chunk_len)\n",
        "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=0, drop_last=True)\n",
        "\n",
        "#Sample parameters, use whatever you see fit.\n",
        "input_dim = train_dataset.n_characters\n",
        "hidden_dim = 256\n",
        "output_dim = train_dataset.n_characters\n",
        "learning_rate = 0.005\n",
        "model = LSTMSimple(chunk_len,input_dim, hidden_dim, output_dim,batch_size)\n",
        "model.train()\n",
        "model.cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "epochs=30\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    with tqdm(total=len(trainloader.dataset), desc ='Training - Epoch: '+str(epoch)+\"/\"+str(epochs), unit='chunks') as prog_bar:\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs = data['input'].float()\n",
        "            labels = data['target'].float()\n",
        "            # b x chunk_len x len(dataset.all_characters)\n",
        "            target = torch.argmax(labels,dim=2)\n",
        "            optimizer.zero_grad()\n",
        "            outputs, _ = model(inputs)\n",
        "            loss = criterion(outputs.view(inputs.shape[0]*inputs.shape[1],-1),target.view(labels.shape[0]*labels.shape[1]))\n",
        "            loss.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(),\n",
        "                                      max_norm=10.0)\n",
        "            optimizer.step()\n",
        "            prog_bar.set_postfix(**{'run:': model_name,'lr': learning_rate,\n",
        "                                    'loss': loss.item()\n",
        "                                    })\n",
        "            prog_bar.update(batch_size)\n",
        "        # Intermediate output\n",
        "        sample_text = \"O Romeo, wherefore art thou\"\n",
        "        inp = train_dataset.char_tensor(sample_text)\n",
        "        sample_input = Variable(inp).cuda().unsqueeze(0).float()\n",
        "        out_test = topk_sampling_lstm(model,sample_input, 300)[0]\n",
        "        out_char_index = torch.argmax(out_test, dim=1).detach().cpu().numpy()\n",
        "        out_chars = sample_text+\"\".join([train_dataset.all_characters[i] for i in out_char_index])\n",
        "        print(\"Top-K sampling -----------------\")\n",
        "        print(out_chars)\n",
        "\n",
        "        out_test = greedy_sampling_lstm(model,sample_input, 300)[0]\n",
        "        out_char_index = torch.argmax(out_test, dim=1).detach().cpu().numpy()\n",
        "        out_chars = sample_text+\"\".join([train_dataset.all_characters[i] for i in out_char_index])\n",
        "        print(\"Greedy sampling ----------------\")\n",
        "        print(out_chars)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NQizlJ4ekXY"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrpyvNDwelEX",
        "outputId": "4974831c-5b21-4f24-ec57-c37409313435"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Training with sequence length: 32\n",
            "============================================================\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 0/30: 100%|█████████▉| 9984/10000 [00:04<00:00, 2067.06chunks/s, loss=3.28, lr=0.005, run:=LSTM_seq32]\n",
            "Training - Epoch: 1/30: 100%|█████████▉| 9984/10000 [00:04<00:00, 2486.98chunks/s, loss=2.71, lr=0.005, run:=LSTM_seq32]\n",
            "Training - Epoch: 2/30: 100%|█████████▉| 9984/10000 [00:04<00:00, 2125.42chunks/s, loss=2.43, lr=0.005, run:=LSTM_seq32]\n",
            "Training - Epoch: 3/30: 100%|█████████▉| 9984/10000 [00:04<00:00, 2430.52chunks/s, loss=2.32, lr=0.005, run:=LSTM_seq32]\n",
            "Training - Epoch: 4/30: 100%|█████████▉| 9984/10000 [00:04<00:00, 2117.28chunks/s, loss=2.23, lr=0.005, run:=LSTM_seq32]\n",
            "Training - Epoch: 5/30: 100%|█████████▉| 9984/10000 [00:04<00:00, 2466.83chunks/s, loss=2.18, lr=0.005, run:=LSTM_seq32]\n",
            "Training - Epoch: 6/30: 100%|█████████▉| 9984/10000 [00:05<00:00, 1982.93chunks/s, loss=2.08, lr=0.005, run:=LSTM_seq32]\n",
            "Training - Epoch: 7/30: 100%|█████████▉| 9984/10000 [00:04<00:00, 2275.31chunks/s, loss=2.04, lr=0.005, run:=LSTM_seq32]\n",
            "Training - Epoch: 8/30: 100%|█████████▉| 9984/10000 [00:04<00:00, 2397.81chunks/s, loss=2, lr=0.005, run:=LSTM_seq32]\n",
            "Training - Epoch: 9/30: 100%|█████████▉| 9984/10000 [00:04<00:00, 2097.36chunks/s, loss=2, lr=0.005, run:=LSTM_seq32]\n",
            "Training - Epoch: 10/30: 100%|█████████▉| 9984/10000 [00:04<00:00, 2483.85chunks/s, loss=1.93, lr=0.005, run:=LSTM_seq32]\n",
            "Training - Epoch: 11/30: 100%|█████████▉| 9984/10000 [00:04<00:00, 2407.56chunks/s, loss=1.88, lr=0.005, run:=LSTM_seq32]\n",
            "Training - Epoch: 12/30: 100%|█████████▉| 9984/10000 [00:04<00:00, 2269.98chunks/s, loss=1.89, lr=0.005, run:=LSTM_seq32]\n",
            "Training - Epoch: 13/30: 100%|█████████▉| 9984/10000 [00:04<00:00, 2492.33chunks/s, loss=1.85, lr=0.005, run:=LSTM_seq32]\n",
            "Training - Epoch: 14/30: 100%|█████████▉| 9984/10000 [00:04<00:00, 2115.87chunks/s, loss=1.82, lr=0.005, run:=LSTM_seq32]\n",
            "Training - Epoch: 15/30: 100%|█████████▉| 9984/10000 [00:04<00:00, 2477.93chunks/s, loss=1.8, lr=0.005, run:=LSTM_seq32]\n",
            "Training - Epoch: 16/30: 100%|█████████▉| 9984/10000 [00:04<00:00, 2319.88chunks/s, loss=1.8, lr=0.005, run:=LSTM_seq32]\n",
            "Training - Epoch: 17/30: 100%|█████████▉| 9984/10000 [00:04<00:00, 2309.36chunks/s, loss=1.79, lr=0.005, run:=LSTM_seq32]\n",
            "Training - Epoch: 18/30: 100%|█████████▉| 9984/10000 [00:04<00:00, 2107.54chunks/s, loss=1.75, lr=0.005, run:=LSTM_seq32]\n",
            "Training - Epoch: 19/30: 100%|█████████▉| 9984/10000 [00:04<00:00, 2114.94chunks/s, loss=1.72, lr=0.005, run:=LSTM_seq32]\n",
            "Training - Epoch: 20/30: 100%|█████████▉| 9984/10000 [00:04<00:00, 2434.60chunks/s, loss=1.73, lr=0.005, run:=LSTM_seq32]\n",
            "Training - Epoch: 21/30: 100%|█████████▉| 9984/10000 [00:04<00:00, 2143.62chunks/s, loss=1.72, lr=0.005, run:=LSTM_seq32]\n",
            "Training - Epoch: 22/30: 100%|█████████▉| 9984/10000 [00:04<00:00, 2488.77chunks/s, loss=1.7, lr=0.005, run:=LSTM_seq32]\n",
            "Training - Epoch: 23/30: 100%|█████████▉| 9984/10000 [00:04<00:00, 2457.30chunks/s, loss=1.68, lr=0.005, run:=LSTM_seq32]\n",
            "Training - Epoch: 24/30: 100%|█████████▉| 9984/10000 [00:04<00:00, 2131.52chunks/s, loss=1.65, lr=0.005, run:=LSTM_seq32]\n",
            "Training - Epoch: 25/30: 100%|█████████▉| 9984/10000 [00:04<00:00, 2493.15chunks/s, loss=1.7, lr=0.005, run:=LSTM_seq32]\n",
            "Training - Epoch: 26/30: 100%|█████████▉| 9984/10000 [00:04<00:00, 2274.35chunks/s, loss=1.64, lr=0.005, run:=LSTM_seq32]\n",
            "Training - Epoch: 27/30: 100%|█████████▉| 9984/10000 [00:05<00:00, 1961.49chunks/s, loss=1.65, lr=0.005, run:=LSTM_seq32]\n",
            "Training - Epoch: 28/30: 100%|█████████▉| 9984/10000 [00:04<00:00, 2471.00chunks/s, loss=1.62, lr=0.005, run:=LSTM_seq32]\n",
            "Training - Epoch: 29/30: 100%|█████████▉| 9984/10000 [00:04<00:00, 2296.41chunks/s, loss=1.65, lr=0.005, run:=LSTM_seq32]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top-K sampling -----------------\n",
            "O Romeo, wherefore art thousand mind anstless\n",
            "Than to mady to-mards, and so set if a that should see to the world\n",
            "A direter our patted mistomsells, the manded wiffock, the mancoman: we him fair soul ot the self\n",
            "To to any so tood too himself is me to hear fall of you.\n",
            "\n",
            "LUCIO:\n",
            "And all me thou heligh'd migh sighters.\n",
            "\n",
            "POMPPYO:\n",
            "N\n",
            "\n",
            "Greedy sampling ----------------\n",
            "O Romeo, wherefore art thou art the stand to the stand to the stand to the stand to the stand\n",
            "That the couse the stand to the stand to the stand to the stand to the stand to the stand to the stand\n",
            "That the couse the stand to the stand to the stand to the stand to the stand to the stand to the stand\n",
            "That the couse the stand to\n",
            "\n",
            "Training completed in 149.63 seconds\n",
            "\n",
            "============================================================\n",
            "Training with sequence length: 64\n",
            "============================================================\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 0/30: 100%|█████████▉| 9984/10000 [00:06<00:00, 1483.43chunks/s, loss=3.23, lr=0.005, run:=LSTM_seq64]\n",
            "Training - Epoch: 1/30: 100%|█████████▉| 9984/10000 [00:06<00:00, 1463.73chunks/s, loss=2.73, lr=0.005, run:=LSTM_seq64]\n",
            "Training - Epoch: 2/30: 100%|█████████▉| 9984/10000 [00:07<00:00, 1366.13chunks/s, loss=2.46, lr=0.005, run:=LSTM_seq64]\n",
            "Training - Epoch: 3/30: 100%|█████████▉| 9984/10000 [00:06<00:00, 1517.98chunks/s, loss=2.32, lr=0.005, run:=LSTM_seq64]\n",
            "Training - Epoch: 4/30: 100%|█████████▉| 9984/10000 [00:07<00:00, 1314.71chunks/s, loss=2.25, lr=0.005, run:=LSTM_seq64]\n",
            "Training - Epoch: 5/30: 100%|█████████▉| 9984/10000 [00:07<00:00, 1388.79chunks/s, loss=2.17, lr=0.005, run:=LSTM_seq64]\n",
            "Training - Epoch: 6/30: 100%|█████████▉| 9984/10000 [00:06<00:00, 1524.19chunks/s, loss=2.1, lr=0.005, run:=LSTM_seq64]\n",
            "Training - Epoch: 7/30: 100%|█████████▉| 9984/10000 [00:07<00:00, 1381.47chunks/s, loss=2.04, lr=0.005, run:=LSTM_seq64]\n",
            "Training - Epoch: 8/30: 100%|█████████▉| 9984/10000 [00:06<00:00, 1453.12chunks/s, loss=2, lr=0.005, run:=LSTM_seq64]\n",
            "Training - Epoch: 9/30: 100%|█████████▉| 9984/10000 [00:06<00:00, 1445.77chunks/s, loss=1.96, lr=0.005, run:=LSTM_seq64]\n",
            "Training - Epoch: 10/30: 100%|█████████▉| 9984/10000 [00:07<00:00, 1371.68chunks/s, loss=1.89, lr=0.005, run:=LSTM_seq64]\n",
            "Training - Epoch: 11/30: 100%|█████████▉| 9984/10000 [00:06<00:00, 1517.90chunks/s, loss=1.85, lr=0.005, run:=LSTM_seq64]\n",
            "Training - Epoch: 12/30: 100%|█████████▉| 9984/10000 [00:07<00:00, 1379.86chunks/s, loss=1.81, lr=0.005, run:=LSTM_seq64]\n",
            "Training - Epoch: 13/30: 100%|█████████▉| 9984/10000 [00:07<00:00, 1398.09chunks/s, loss=1.8, lr=0.005, run:=LSTM_seq64]\n",
            "Training - Epoch: 14/30: 100%|█████████▉| 9984/10000 [00:06<00:00, 1558.15chunks/s, loss=1.77, lr=0.005, run:=LSTM_seq64]\n",
            "Training - Epoch: 15/30: 100%|█████████▉| 9984/10000 [00:07<00:00, 1415.40chunks/s, loss=1.74, lr=0.005, run:=LSTM_seq64]\n",
            "Training - Epoch: 16/30: 100%|█████████▉| 9984/10000 [00:06<00:00, 1543.22chunks/s, loss=1.75, lr=0.005, run:=LSTM_seq64]\n",
            "Training - Epoch: 17/30: 100%|█████████▉| 9984/10000 [00:06<00:00, 1463.43chunks/s, loss=1.72, lr=0.005, run:=LSTM_seq64]\n",
            "Training - Epoch: 18/30: 100%|█████████▉| 9984/10000 [00:07<00:00, 1393.22chunks/s, loss=1.69, lr=0.005, run:=LSTM_seq64]\n",
            "Training - Epoch: 19/30: 100%|█████████▉| 9984/10000 [00:06<00:00, 1525.33chunks/s, loss=1.71, lr=0.005, run:=LSTM_seq64]\n",
            "Training - Epoch: 20/30: 100%|█████████▉| 9984/10000 [00:07<00:00, 1380.32chunks/s, loss=1.69, lr=0.005, run:=LSTM_seq64]\n",
            "Training - Epoch: 21/30: 100%|█████████▉| 9984/10000 [00:07<00:00, 1367.92chunks/s, loss=1.65, lr=0.005, run:=LSTM_seq64]\n",
            "Training - Epoch: 22/30: 100%|█████████▉| 9984/10000 [00:07<00:00, 1376.18chunks/s, loss=1.61, lr=0.005, run:=LSTM_seq64]\n",
            "Training - Epoch: 23/30: 100%|█████████▉| 9984/10000 [00:07<00:00, 1380.68chunks/s, loss=1.64, lr=0.005, run:=LSTM_seq64]\n",
            "Training - Epoch: 24/30: 100%|█████████▉| 9984/10000 [00:06<00:00, 1466.33chunks/s, loss=1.65, lr=0.005, run:=LSTM_seq64]\n",
            "Training - Epoch: 25/30: 100%|█████████▉| 9984/10000 [00:06<00:00, 1473.48chunks/s, loss=1.61, lr=0.005, run:=LSTM_seq64]\n",
            "Training - Epoch: 26/30: 100%|█████████▉| 9984/10000 [00:07<00:00, 1390.67chunks/s, loss=1.61, lr=0.005, run:=LSTM_seq64]\n",
            "Training - Epoch: 27/30: 100%|█████████▉| 9984/10000 [00:06<00:00, 1544.39chunks/s, loss=1.61, lr=0.005, run:=LSTM_seq64]\n",
            "Training - Epoch: 28/30: 100%|█████████▉| 9984/10000 [00:07<00:00, 1384.74chunks/s, loss=1.59, lr=0.005, run:=LSTM_seq64]\n",
            "Training - Epoch: 29/30: 100%|█████████▉| 9984/10000 [00:07<00:00, 1385.55chunks/s, loss=1.58, lr=0.005, run:=LSTM_seq64]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top-K sampling -----------------\n",
            "O Romeo, wherefore art thou are hene.\n",
            "\n",
            "PERDWIV:\n",
            "A grant, and we come who can our grand to stane\n",
            "And wordor of the part him thing like\n",
            "Of these be a man one and morn.\n",
            "\n",
            "KATHARINA:\n",
            "He ware sauness to may the cray.\n",
            "What, these should be son, say, a would have back?\n",
            "\n",
            "Secold Gown:\n",
            "Ay, my lord,\n",
            "I am a care, that while is an hand the\n",
            "\n",
            "Greedy sampling ----------------\n",
            "O Romeo, wherefore art thou hast the send the see\n",
            "The sent the send the send the send the send the send\n",
            "The sent the sent the send the send the send the send the send the send the send the send the send the send the send the send the send the send the send the send the send the send the send the send the send the send the sen\n",
            "\n",
            "Training completed in 226.16 seconds\n",
            "\n",
            "============================================================\n",
            "Training with sequence length: 128\n",
            "============================================================\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 0/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 820.02chunks/s, loss=3.27, lr=0.005, run:=LSTM_seq128]\n",
            "Training - Epoch: 1/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 815.00chunks/s, loss=2.8, lr=0.005, run:=LSTM_seq128]\n",
            "Training - Epoch: 2/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 809.29chunks/s, loss=2.46, lr=0.005, run:=LSTM_seq128]\n",
            "Training - Epoch: 3/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 819.91chunks/s, loss=2.33, lr=0.005, run:=LSTM_seq128]\n",
            "Training - Epoch: 4/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 824.47chunks/s, loss=2.24, lr=0.005, run:=LSTM_seq128]\n",
            "Training - Epoch: 5/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 822.61chunks/s, loss=2.18, lr=0.005, run:=LSTM_seq128]\n",
            "Training - Epoch: 6/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 821.61chunks/s, loss=2.1, lr=0.005, run:=LSTM_seq128]\n",
            "Training - Epoch: 7/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 821.05chunks/s, loss=2.07, lr=0.005, run:=LSTM_seq128]\n",
            "Training - Epoch: 8/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 812.21chunks/s, loss=2.01, lr=0.005, run:=LSTM_seq128]\n",
            "Training - Epoch: 9/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 821.49chunks/s, loss=1.94, lr=0.005, run:=LSTM_seq128]\n",
            "Training - Epoch: 10/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 810.59chunks/s, loss=1.91, lr=0.005, run:=LSTM_seq128]\n",
            "Training - Epoch: 11/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 805.30chunks/s, loss=1.88, lr=0.005, run:=LSTM_seq128]\n",
            "Training - Epoch: 12/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 788.59chunks/s, loss=1.82, lr=0.005, run:=LSTM_seq128]\n",
            "Training - Epoch: 13/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 782.33chunks/s, loss=1.82, lr=0.005, run:=LSTM_seq128]\n",
            "Training - Epoch: 14/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 795.87chunks/s, loss=1.79, lr=0.005, run:=LSTM_seq128]\n",
            "Training - Epoch: 15/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 820.52chunks/s, loss=1.76, lr=0.005, run:=LSTM_seq128]\n",
            "Training - Epoch: 16/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 812.12chunks/s, loss=1.73, lr=0.005, run:=LSTM_seq128]\n",
            "Training - Epoch: 17/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 817.92chunks/s, loss=1.72, lr=0.005, run:=LSTM_seq128]\n",
            "Training - Epoch: 18/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 821.48chunks/s, loss=1.7, lr=0.005, run:=LSTM_seq128]\n",
            "Training - Epoch: 19/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 819.59chunks/s, loss=1.66, lr=0.005, run:=LSTM_seq128]\n",
            "Training - Epoch: 20/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 823.05chunks/s, loss=1.66, lr=0.005, run:=LSTM_seq128]\n",
            "Training - Epoch: 21/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 815.39chunks/s, loss=1.65, lr=0.005, run:=LSTM_seq128]\n",
            "Training - Epoch: 22/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 816.66chunks/s, loss=1.63, lr=0.005, run:=LSTM_seq128]\n",
            "Training - Epoch: 23/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 810.37chunks/s, loss=1.61, lr=0.005, run:=LSTM_seq128]\n",
            "Training - Epoch: 24/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 812.10chunks/s, loss=1.62, lr=0.005, run:=LSTM_seq128]\n",
            "Training - Epoch: 25/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 804.78chunks/s, loss=1.6, lr=0.005, run:=LSTM_seq128]\n",
            "Training - Epoch: 26/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 803.57chunks/s, loss=1.56, lr=0.005, run:=LSTM_seq128]\n",
            "Training - Epoch: 27/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 804.55chunks/s, loss=1.58, lr=0.005, run:=LSTM_seq128]\n",
            "Training - Epoch: 28/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 796.67chunks/s, loss=1.56, lr=0.005, run:=LSTM_seq128]\n",
            "Training - Epoch: 29/30: 100%|█████████▉| 9984/10000 [00:12<00:00, 786.94chunks/s, loss=1.54, lr=0.005, run:=LSTM_seq128]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top-K sampling -----------------\n",
            "O Romeo, wherefore art thousted and see\n",
            "thee that I do me to he well me, and here\n",
            "Is for the sonest on the both the seak have.\n",
            "\n",
            "KATHAPSOr:\n",
            "Was you are boon you and tently are that then tow\n",
            "The sent the mentied wother heaven some too man, brink to morn\n",
            "I will more the more and so lady.\n",
            "Take honest he should me will thy said an\n",
            "\n",
            "Greedy sampling ----------------\n",
            "O Romeo, wherefore art thou art the stands the some\n",
            "That should the come to the come to the come to the country with the send\n",
            "That shall be the come to the come to the country with the send\n",
            "That shall be the come to the come to the country with the send\n",
            "That shall be the come to the come to the country with the send\n",
            "That shal\n",
            "\n",
            "Training completed in 386.77 seconds\n",
            "\n",
            "============================================================\n",
            "Training with sequence length: 256\n",
            "============================================================\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 0/30: 100%|█████████▉| 9984/10000 [00:23<00:00, 417.02chunks/s, loss=3.26, lr=0.005, run:=LSTM_seq256]\n",
            "Training - Epoch: 1/30: 100%|█████████▉| 9984/10000 [00:23<00:00, 429.79chunks/s, loss=2.83, lr=0.005, run:=LSTM_seq256]\n",
            "Training - Epoch: 2/30: 100%|█████████▉| 9984/10000 [00:23<00:00, 430.75chunks/s, loss=2.47, lr=0.005, run:=LSTM_seq256]\n",
            "Training - Epoch: 3/30: 100%|█████████▉| 9984/10000 [00:23<00:00, 423.13chunks/s, loss=2.36, lr=0.005, run:=LSTM_seq256]\n",
            "Training - Epoch: 4/30: 100%|█████████▉| 9984/10000 [00:23<00:00, 425.04chunks/s, loss=2.26, lr=0.005, run:=LSTM_seq256]\n",
            "Training - Epoch: 5/30: 100%|█████████▉| 9984/10000 [00:23<00:00, 425.06chunks/s, loss=2.2, lr=0.005, run:=LSTM_seq256]\n",
            "Training - Epoch: 6/30: 100%|█████████▉| 9984/10000 [00:23<00:00, 425.98chunks/s, loss=2.14, lr=0.005, run:=LSTM_seq256]\n",
            "Training - Epoch: 7/30: 100%|█████████▉| 9984/10000 [00:22<00:00, 435.29chunks/s, loss=2.11, lr=0.005, run:=LSTM_seq256]\n",
            "Training - Epoch: 8/30: 100%|█████████▉| 9984/10000 [00:22<00:00, 434.95chunks/s, loss=2.02, lr=0.005, run:=LSTM_seq256]\n",
            "Training - Epoch: 9/30: 100%|█████████▉| 9984/10000 [00:24<00:00, 414.63chunks/s, loss=1.98, lr=0.005, run:=LSTM_seq256]\n",
            "Training - Epoch: 10/30: 100%|█████████▉| 9984/10000 [00:23<00:00, 427.59chunks/s, loss=1.91, lr=0.005, run:=LSTM_seq256]\n",
            "Training - Epoch: 11/30: 100%|█████████▉| 9984/10000 [00:23<00:00, 420.10chunks/s, loss=1.88, lr=0.005, run:=LSTM_seq256]\n",
            "Training - Epoch: 12/30: 100%|█████████▉| 9984/10000 [00:23<00:00, 426.93chunks/s, loss=1.83, lr=0.005, run:=LSTM_seq256]\n",
            "Training - Epoch: 13/30: 100%|█████████▉| 9984/10000 [00:23<00:00, 430.53chunks/s, loss=1.8, lr=0.005, run:=LSTM_seq256]\n",
            "Training - Epoch: 14/30: 100%|█████████▉| 9984/10000 [00:23<00:00, 430.10chunks/s, loss=1.78, lr=0.005, run:=LSTM_seq256]\n",
            "Training - Epoch: 15/30: 100%|█████████▉| 9984/10000 [00:23<00:00, 431.50chunks/s, loss=1.75, lr=0.005, run:=LSTM_seq256]\n",
            "Training - Epoch: 16/30: 100%|█████████▉| 9984/10000 [00:23<00:00, 432.04chunks/s, loss=1.71, lr=0.005, run:=LSTM_seq256]\n",
            "Training - Epoch: 17/30: 100%|█████████▉| 9984/10000 [00:23<00:00, 429.07chunks/s, loss=1.69, lr=0.005, run:=LSTM_seq256]\n",
            "Training - Epoch: 18/30: 100%|█████████▉| 9984/10000 [00:23<00:00, 423.49chunks/s, loss=1.7, lr=0.005, run:=LSTM_seq256]\n",
            "Training - Epoch: 19/30: 100%|█████████▉| 9984/10000 [00:22<00:00, 442.61chunks/s, loss=1.68, lr=0.005, run:=LSTM_seq256]\n",
            "Training - Epoch: 20/30: 100%|█████████▉| 9984/10000 [00:22<00:00, 436.52chunks/s, loss=1.64, lr=0.005, run:=LSTM_seq256]\n",
            "Training - Epoch: 21/30: 100%|█████████▉| 9984/10000 [00:23<00:00, 428.55chunks/s, loss=1.62, lr=0.005, run:=LSTM_seq256]\n",
            "Training - Epoch: 22/30: 100%|█████████▉| 9984/10000 [00:23<00:00, 432.40chunks/s, loss=1.62, lr=0.005, run:=LSTM_seq256]\n",
            "Training - Epoch: 23/30: 100%|█████████▉| 9984/10000 [00:23<00:00, 428.59chunks/s, loss=1.59, lr=0.005, run:=LSTM_seq256]\n",
            "Training - Epoch: 24/30: 100%|█████████▉| 9984/10000 [00:23<00:00, 433.65chunks/s, loss=1.61, lr=0.005, run:=LSTM_seq256]\n",
            "Training - Epoch: 25/30: 100%|█████████▉| 9984/10000 [00:22<00:00, 438.07chunks/s, loss=1.58, lr=0.005, run:=LSTM_seq256]\n",
            "Training - Epoch: 26/30: 100%|█████████▉| 9984/10000 [00:23<00:00, 432.83chunks/s, loss=1.55, lr=0.005, run:=LSTM_seq256]\n",
            "Training - Epoch: 27/30: 100%|█████████▉| 9984/10000 [00:23<00:00, 420.96chunks/s, loss=1.54, lr=0.005, run:=LSTM_seq256]\n",
            "Training - Epoch: 28/30: 100%|█████████▉| 9984/10000 [00:23<00:00, 432.84chunks/s, loss=1.53, lr=0.005, run:=LSTM_seq256]\n",
            "Training - Epoch: 29/30: 100%|█████████▉| 9984/10000 [00:23<00:00, 430.44chunks/s, loss=1.55, lr=0.005, run:=LSTM_seq256]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top-K sampling -----------------\n",
            "O Romeo, wherefore art thou haven that's take have that\n",
            "There is thee, make a please of his peacian\n",
            "A spile of here hath his brow the works. Though him:\n",
            "Allow they sige it is a man to sempose\n",
            "But thee in the seight and brow wasers on\n",
            "As for to my hinde in timen all mother\n",
            "As the stine of his fear men.\n",
            "\n",
            "PETRUCHIO:\n",
            "I would's th\n",
            "\n",
            "Greedy sampling ----------------\n",
            "O Romeo, wherefore art thou do the stander of the son\n",
            "To see the stander of the son the proper and thee,\n",
            "And the proper the stander of the sonester\n",
            "That when the stander the stander of the son\n",
            "To see the stander the stander of the son\n",
            "To see the stander the stander of the son\n",
            "To see the stander the stander of the son\n",
            "To see t\n",
            "\n",
            "Training completed in 718.04 seconds\n",
            "\n",
            "Experiment completed. Results saved to the 'results' directory.\n",
            "\n",
            "Summary of Final Losses:\n",
            "Sequence Length 32: Final Loss = 1.6354\n",
            "Sequence Length 64: Final Loss = 1.5809\n",
            "Sequence Length 128: Final Loss = 1.5542\n",
            "Sequence Length 256: Final Loss = 1.5320\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import time\n",
        "\n",
        "def train_with_sequence_length(seq_length, epochs=10, hidden_dim=256, batch_size=256, learning_rate=0.005):\n",
        "\n",
        "    os.makedirs(\"results\", exist_ok=True)\n",
        "\n",
        "\n",
        "    train_dataset = LSTMDataset(chunk_len=seq_length)\n",
        "    trainloader = torch.utils.data.DataLoader(\n",
        "        train_dataset, batch_size=batch_size, num_workers=0, drop_last=True\n",
        "    )\n",
        "\n",
        "\n",
        "    input_dim = train_dataset.n_characters\n",
        "    output_dim = train_dataset.n_characters\n",
        "    model_name = f\"LSTM_seq{seq_length}\"\n",
        "\n",
        "\n",
        "    model = LSTMSimple(seq_length, input_dim, hidden_dim, output_dim, batch_size)\n",
        "\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    model.train()\n",
        "\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "    losses = []\n",
        "    samples = {\"topk\": [], \"greedy\": []}\n",
        "\n",
        "    print(f\"\\n{'=' * 60}\")\n",
        "    print(f\"Training with sequence length: {seq_length}\")\n",
        "    print(f\"{'=' * 60}\\n\")\n",
        "\n",
        "\n",
        "    start_time = time.time()\n",
        "    for epoch in range(epochs):\n",
        "        epoch_losses = []\n",
        "\n",
        "        with tqdm(total=len(trainloader.dataset), desc=f'Training - Epoch: {epoch}/{epochs}', unit='chunks') as prog_bar:\n",
        "            for i, data in enumerate(trainloader, 0):\n",
        "                # Move data to device\n",
        "                inputs = data['input'].float().to(device)\n",
        "                labels = data['target'].float().to(device)\n",
        "\n",
        "                # Get target indices\n",
        "                target = torch.argmax(labels, dim=2)\n",
        "\n",
        "                # Forward pass\n",
        "                optimizer.zero_grad()\n",
        "                outputs, _ = model(inputs)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = criterion(\n",
        "                    outputs.view(inputs.shape[0] * inputs.shape[1], -1),\n",
        "                    target.view(labels.shape[0] * labels.shape[1])\n",
        "                )\n",
        "\n",
        "\n",
        "                loss.backward()\n",
        "\n",
        "                # Gradient clipping\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)\n",
        "                # Update parameters\n",
        "                optimizer.step()\n",
        "\n",
        "\n",
        "                epoch_losses.append(loss.item())\n",
        "\n",
        "\n",
        "                prog_bar.set_postfix(**{\n",
        "                    'run:': model_name,\n",
        "                    'lr': learning_rate,\n",
        "                    'loss': loss.item()\n",
        "                })\n",
        "                prog_bar.update(batch_size)\n",
        "\n",
        "        # Average loss for this epoch\n",
        "        avg_epoch_loss = sum(epoch_losses) / len(epoch_losses)\n",
        "        losses.append(avg_epoch_loss)\n",
        "\n",
        "        # Generate samples at the end of each epoch\n",
        "        sample_text = \"O Romeo, wherefore art thou\"\n",
        "        inp = train_dataset.char_tensor(sample_text)\n",
        "        sample_input = Variable(inp).to(device).unsqueeze(0).float()\n",
        "\n",
        "        # Generate text with top-k sampling\n",
        "        out_test_topk = topk_sampling_lstm(model, sample_input, 300)[0]\n",
        "        out_char_index_topk = torch.argmax(out_test_topk, dim=1).detach().cpu().numpy()\n",
        "        out_chars_topk = sample_text + \"\".join([train_dataset.all_characters[i] for i in out_char_index_topk])\n",
        "        samples[\"topk\"].append(out_chars_topk)\n",
        "\n",
        "        # Generate text with greedy sampling\n",
        "        out_test_greedy = greedy_sampling_lstm(model, sample_input, 300)[0]\n",
        "        out_char_index_greedy = torch.argmax(out_test_greedy, dim=1).detach().cpu().numpy()\n",
        "        out_chars_greedy = sample_text + \"\".join([train_dataset.all_characters[i] for i in out_char_index_greedy])\n",
        "        samples[\"greedy\"].append(out_chars_greedy)\n",
        "\n",
        "\n",
        "        if epoch == epochs - 1:\n",
        "            print(\"\\nTop-K sampling -----------------\")\n",
        "            print(out_chars_topk)\n",
        "            print(\"\\nGreedy sampling ----------------\")\n",
        "            print(out_chars_greedy)\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "    print(f\"\\nTraining completed in {training_time:.2f} seconds\")\n",
        "\n",
        "\n",
        "    torch.save(model.state_dict(), f\"results/lstm_seq{seq_length}.pth\")\n",
        "\n",
        "    return model, losses, samples\n",
        "\n",
        "\n",
        "def run_sequence_length_experiment(sequence_lengths, epochs=10, hidden_dim=256, batch_size=256, learning_rate=0.005):\n",
        "    \"\"\"\n",
        "    Run experiment with multiple sequence lengths\n",
        "\n",
        "    Args:\n",
        "        sequence_lengths: List of sequence lengths to train with\n",
        "        epochs: Number of epochs for each training run\n",
        "        hidden_dim: Hidden dimension size\n",
        "        batch_size: Batch size for training\n",
        "        learning_rate: Learning rate for optimization\n",
        "    \"\"\"\n",
        "\n",
        "    all_results = {}\n",
        "\n",
        "    for seq_len in sequence_lengths:\n",
        "        # Train with this sequence length\n",
        "        model, losses, samples = train_with_sequence_length(\n",
        "            seq_length=seq_len,\n",
        "            epochs=epochs,\n",
        "            hidden_dim=hidden_dim,\n",
        "            batch_size=batch_size,\n",
        "            learning_rate=learning_rate\n",
        "        )\n",
        "\n",
        "        # Store results\n",
        "        all_results[seq_len] = {\n",
        "            \"model\": model,\n",
        "            \"losses\": losses,\n",
        "            \"samples\": samples\n",
        "        }\n",
        "\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    for seq_len, results in all_results.items():\n",
        "        plt.plot(results[\"losses\"], label=f\"Sequence Length: {seq_len}\")\n",
        "\n",
        "    plt.title(\"Training Loss vs. Epoch for Different Sequence Lengths\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig(\"results/sequence_length_loss_comparison.png\")\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "    with open(\"results/sequence_length_samples.txt\", \"w\") as f:\n",
        "        f.write(\"Generated Text Samples with Different Sequence Lengths\\n\")\n",
        "        f.write(\"=\" * 60 + \"\\n\\n\")\n",
        "\n",
        "        for seq_len, results in all_results.items():\n",
        "            f.write(f\"Sequence Length: {seq_len}\\n\")\n",
        "            f.write(\"-\" * 40 + \"\\n\\n\")\n",
        "\n",
        "            # Get the final samples (from the last epoch)\n",
        "            final_topk = results[\"samples\"][\"topk\"][-1]\n",
        "            final_greedy = results[\"samples\"][\"greedy\"][-1]\n",
        "\n",
        "            f.write(\"Top-K Sampling:\\n\")\n",
        "            f.write(final_topk + \"\\n\\n\")\n",
        "\n",
        "            f.write(\"Greedy Sampling:\\n\")\n",
        "            f.write(final_greedy + \"\\n\\n\")\n",
        "\n",
        "            f.write(\"\\n\" + \"=\" * 60 + \"\\n\\n\")\n",
        "\n",
        "    print(\"\\nExperiment completed. Results saved to the 'results' directory.\")\n",
        "    return all_results\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "sequence_lengths = [32, 64, 128, 256]\n",
        "\n",
        "epochs = 30\n",
        "\n",
        "results = run_sequence_length_experiment(\n",
        "        sequence_lengths=sequence_lengths,\n",
        "        epochs=epochs,\n",
        "        hidden_dim=256,\n",
        "        batch_size=256,\n",
        "        learning_rate=0.005\n",
        "    )\n",
        "\n",
        "print(\"\\nSummary of Final Losses:\")\n",
        "for seq_len, res in results.items():\n",
        "      print(f\"Sequence Length {seq_len}: Final Loss = {res['losses'][-1]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwQEM1JHzDo-"
      },
      "source": [
        "# Task 2: Character generation transformer network implementation\n",
        "Our simple transformer-like network will take as input a sequence of characters and predict the next character in the sequence. To ensure an efficient training procedure, masked attention modules will be used as in the [GPT model](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf).\n",
        "\n",
        "For this task you must implement the Scaled dot product attention module and the Masked multi-head attention module. Both of these modules are described in the [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf) paper (See Figure 2 in the paper as well as Sections 3.2.1, 3.2.2 and 3.2.3). They are the core operations of transformers. As we will use our model for text generation also add the masking operation shown as (mask opt.) in Figure 2, implemented as AttentionMasking in the code.\n",
        "\n",
        "**Implement the modules in the ScaledDotProductAttention class and the MultiHeadAttention class.**\n",
        "\n",
        "Read the GPT paper and the Attention is all you need paper for a better understanding of the components. For a more high level overview, this [post](https://jalammar.github.io/illustrated-gpt2/) may also be helpful.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "J2JUunhca2Nz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import math\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=1000):\n",
        "        super().__init__()\n",
        "        position = torch.arange(max_len).unsqueeze(1).float()\n",
        "        div_term = 10000.0**(torch.arange(0,d_model,2).float()/d_model)\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        pe[:, 0::2] = torch.sin(position / div_term)\n",
        "        pe[:, 1::2] = torch.cos(position / div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        # Check for CUDA availability and move tensor accordingly\n",
        "        if torch.cuda.is_available():\n",
        "            self.pe = pe.cuda()\n",
        "        else:\n",
        "            self.pe = pe  # Keep tensor on CPU if CUDA is not available\n",
        "        self.pe.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        p = self.pe[:, :x.size(1)]\n",
        "        return p\n",
        "\n",
        "class AttentionMasking(nn.Module):\n",
        "    def __init__(self, max_len):\n",
        "        super(AttentionMasking, self).__init__()\n",
        "        mask = torch.tril(torch.ones(max_len, max_len)).view(1, 1, max_len, max_len)\n",
        "        # register_buffer will move tensor to the right device when model.to(device) is called\n",
        "        self.register_buffer(\"mask\", mask)\n",
        "\n",
        "    def forward(self,x):\n",
        "        length = x.shape[-1]\n",
        "        out = x.masked_fill(self.mask[:,:,:length,:length] == 0, float('-inf'))\n",
        "        return out\n",
        "\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, max_len, dropout_rate=0.1):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        # Multiply with an upper triangular\n",
        "        # matrix of dimensions (length x length) after the scale operation\n",
        "        # in Figure 2 of the paper.\n",
        "        self.mask_opt = AttentionMasking(max_len)\n",
        "        # Added dropout as per the paper\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self,q,k,v):\n",
        "        # length = number of input tokens\n",
        "        batch_size,num_heads,length,num_neuron = k.size()\n",
        "        # Step 1: MatMul Q and K^T\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1))\n",
        "        # Step 2: Scale the dot products by 1/sqrt(d_k)\n",
        "        scores = scores / math.sqrt(num_neuron)\n",
        "        # Step 3: Apply mask (optional for decoder self-attention)\n",
        "        scores = self.mask_opt(scores)\n",
        "        attention_weights = self.softmax(scores)\n",
        "        attention_weights = self.dropout(attention_weights)\n",
        "        output = torch.matmul(attention_weights, v)\n",
        "\n",
        "        return output\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, dim_model, num_neuron, n_head, max_len, dropout_rate=0.1):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.n_head = n_head\n",
        "        self.num_neuron = num_neuron\n",
        "\n",
        "        # Initialize the linear projections for Q, K, V\n",
        "        self.W_q = nn.Linear(dim_model, n_head * num_neuron)\n",
        "        self.W_k = nn.Linear(dim_model, n_head * num_neuron)\n",
        "        self.W_v = nn.Linear(dim_model, n_head * num_neuron)\n",
        "\n",
        "        # Final output projection\n",
        "        self.W_o = nn.Linear(n_head * num_neuron, dim_model)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.attention = ScaledDotProductAttention(max_len, dropout_rate)\n",
        "\n",
        "    def split(self,tensor):\n",
        "        batch_size, length, total_dim = tensor.size()\n",
        "        split_tensor = tensor.view(batch_size, length, self.n_head, self.num_neuron).transpose(1,2)\n",
        "        return split_tensor\n",
        "\n",
        "    def concat(self,tensor):\n",
        "        batch_size, num_heads, length, num_neuron = tensor.size()\n",
        "        concat_tensor = tensor.transpose(1,2).contiguous().view(batch_size, length, self.n_head*self.num_neuron)\n",
        "        return concat_tensor\n",
        "\n",
        "    def forward(self, q, k, v):\n",
        "        batch_size = q.size(0)\n",
        "\n",
        "        # Step 1: Linear projections\n",
        "        q_proj = self.W_q(q)  # (batch_size, length, n_head * num_neuron)\n",
        "        k_proj = self.W_k(k)  # (batch_size, length, n_head * num_neuron)\n",
        "        v_proj = self.W_v(v)  # (batch_size, length, n_head * num_neuron)\n",
        "\n",
        "        # Step 2: Split into multiple heads\n",
        "        q_split = self.split(q_proj)  # (batch_size, n_head, length, num_neuron)\n",
        "        k_split = self.split(k_proj)  # (batch_size, n_head, length, num_neuron)\n",
        "        v_split = self.split(v_proj)  # (batch_size, n_head, length, num_neuron)\n",
        "\n",
        "        # Step 3: Apply scaled dot-product attention\n",
        "        attn_output = self.attention(q_split, k_split, v_split)\n",
        "\n",
        "        # Step 4: Concatenate heads\n",
        "        concat_output = self.concat(attn_output)\n",
        "\n",
        "        # Step 5: Apply final linear projection\n",
        "        output = self.W_o(concat_output)\n",
        "\n",
        "        # Step 6: Apply dropout to the output\n",
        "        output = self.dropout(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "class PositionFeedForwardNet(nn.Module):\n",
        "    def __init__(self, dim_model, dropout_rate=0.1):\n",
        "        super(PositionFeedForwardNet, self).__init__()\n",
        "        self.ff_net1 = nn.Linear(dim_model, dim_model*4)\n",
        "        self.ff_net2 = nn.Linear(dim_model*4, dim_model)\n",
        "        # Added dropout as per the paper\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self,x):\n",
        "        ff_out = self.ff_net1(x)\n",
        "        ff_out = torch.nn.functional.relu(ff_out)\n",
        "        ff_out = self.dropout(ff_out)  # Apply dropout after activation\n",
        "        ff_out = self.ff_net2(ff_out)\n",
        "        return ff_out\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, dim_model, num_neuron, n_head, max_len, dropout_rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.mha = MultiHeadAttention(dim_model, num_neuron, n_head, max_len, dropout_rate)\n",
        "        self.l_norm = torch.nn.LayerNorm(dim_model)\n",
        "        self.l_norm2 = torch.nn.LayerNorm(dim_model)\n",
        "        self.ff_net = PositionFeedForwardNet(dim_model, dropout_rate)\n",
        "        # Added dropout as per the paper\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        # b, len_seq, n_head, num_neuron\n",
        "\n",
        "    def forward(self, x):\n",
        "        _x = x\n",
        "        mha1 = self.mha(x,x,x)\n",
        "        lnorm = self.l_norm(_x + mha1)\n",
        "        _x = lnorm\n",
        "        ff_out = self.ff_net(lnorm)\n",
        "        #  residual connection and layer normalization\n",
        "        out = self.l_norm2(_x + ff_out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class TransformerSimple(nn.Module):\n",
        "    def __init__(self, seq_length, input_dim, output_dim, batch_size, dropout_rate=0.1):\n",
        "        super(TransformerSimple, self).__init__()\n",
        "        num_neuron = 64\n",
        "        n_head = 8\n",
        "        dim_model=256\n",
        "        max_len = 512\n",
        "        self.start_embedding = nn.Embedding(input_dim, dim_model)\n",
        "        self.pos_embedding = PositionalEncoding(dim_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Track device to use throughout the model\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.transformer_blocks = nn.ModuleList([\n",
        "            TransformerBlock(dim_model, num_neuron, n_head, max_len, dropout_rate)\n",
        "            for _ in range(5)\n",
        "        ])\n",
        "\n",
        "        self.output_layer = nn.Linear(dim_model, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x - Tensor - (b, seq_len)\n",
        "        # Embeds the input tensor from tokens to features\n",
        "        s_emb = self.start_embedding(x)\n",
        "        # Adds positional embeddings\n",
        "        p_emb = self.pos_embedding(s_emb)\n",
        "        b_out = p_emb + s_emb\n",
        "        # Apply dropout to the combined embeddings\n",
        "        b_out = self.dropout(b_out)\n",
        "\n",
        "        for block in self.transformer_blocks:\n",
        "            b_out = block(b_out)\n",
        "\n",
        "        out = self.output_layer(b_out)\n",
        "\n",
        "        return out\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPjl6ttzPHsq"
      },
      "source": [
        "## Dataset class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNUIBs0CJPEn",
        "outputId": "a1ea92ab-ef43-4685-b8e9-6a497e2e96f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.4.0-py3-none-any.whl.metadata (13 kB)\n",
            "Downloading Unidecode-1.4.0-py3-none-any.whl (235 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/235.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.8/235.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install unidecode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "T04eYePr8Gn3"
      },
      "outputs": [],
      "source": [
        "import unidecode\n",
        "import string\n",
        "import random\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, chunk_len=200, padded_chunks=False):\n",
        "        # Character based dataset\n",
        "        dataset_path = \"./input.txt\"\n",
        "        # The tokens in the vocabulary (all_characters)\n",
        "        # are just the printable characters of the string class\n",
        "        self.all_characters = string.printable\n",
        "        self.n_characters = len(self.all_characters)\n",
        "        # Maps characters to indices\n",
        "        self.char_dict = {x:i for i,x in enumerate(self.all_characters)}\n",
        "        self.file, self.file_len = self.read_file(dataset_path)\n",
        "        # Sequence length of the input\n",
        "        self.chunk_len = chunk_len\n",
        "        self.encoded_file = [self.char_dict[x] for x in self.file]\n",
        "\n",
        "    def read_file(self,filename):\n",
        "        file = unidecode.unidecode(open(filename).read())\n",
        "        return file, len(file)\n",
        "\n",
        "    def encode_text(self,in_str):\n",
        "        # in_str - input sequence - String\n",
        "        # Returns - in_str mapped to tokens in char_dict\n",
        "        tensor = torch.LongTensor([self.char_dict[x] for x in in_str])\n",
        "        return tensor\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inp, target = self.get_random_text()\n",
        "        return {\"input\":inp, \"target\":target}\n",
        "\n",
        "    def __len__(self):\n",
        "        return 10000\n",
        "\n",
        "    def get_random_text(self):\n",
        "        # Pick a random string of length self.chunk_len from the dataset\n",
        "        start_index = np.random.randint(0, self.file_len - self.chunk_len)\n",
        "        end_index = start_index + self.chunk_len + 1\n",
        "        chunk = self.encoded_file[start_index:end_index]\n",
        "        # input_tokens - random sequence of tokens from the dataset\n",
        "        input_tokens = torch.LongTensor(chunk[:-1])\n",
        "        # target - input token sequence shifted by 1\n",
        "        # the idea is to predict next token for each token in the input sequence\n",
        "        # therefore if the input is [1,2,3,4] the target is [2,3,4,5]\n",
        "        target = torch.LongTensor(chunk[1:])\n",
        "        input_tokens = input_tokens.cuda()\n",
        "        target = target.cuda()\n",
        "        return input_tokens, target\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhvayVAjsCMf"
      },
      "source": [
        "## Character sampling\n",
        "\n",
        "To generate text the network must predict the next character in a sequence, however networks do not produce a single character but rather estimate the likelihood for each possible character. Sampling characters from the network output can be done in different ways with common ones being the Greedy sampling process and Top-K sampling.\n",
        "\n",
        "In the simple greedy sampling method the network takes a text prompt as input and generates an additional N tokens by always taking the token with the highest prediction score as the next token.\n",
        "\n",
        "In the Top-K sampling, randomness is added to the sampling process as the network samples from K most likely predicitons at each step. This alleviates the problem of generative models repeating text but may generate incorrect text by sampling inappropriate tokens.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3IVliOUqqEd5"
      },
      "outputs": [],
      "source": [
        "def topk_sampling_iter_transformer(model, x, num_chars, chunk_len, output_token):\n",
        "    # x -- b x onehot_char\n",
        "    # x = b x l\n",
        "    outputs = torch.zeros((1,num_chars))\n",
        "    inp = x\n",
        "\n",
        "    for t in range(num_chars):\n",
        "        # b x onehot_char\n",
        "        output = model(inp.long())[0,-1:]\n",
        "        #output = torch.softmax(output, dim=1)\n",
        "        # b x 3\n",
        "        output_vals, output_ind = torch.topk(output, 5, dim=1)\n",
        "        # 3 -> int\n",
        "        output_vals = torch.softmax(output_vals, dim=1)\n",
        "        top_ind = torch.multinomial(output_vals[0], 1)[0]\n",
        "        # int\n",
        "        out_char_index = output_ind[0,top_ind]\n",
        "        # int -> 1\n",
        "        out_char_index = torch.ones(1).cuda() * out_char_index\n",
        "\n",
        "        outputs[:,t] = out_char_index.item()\n",
        "        if inp.shape[1] > chunk_len:\n",
        "          inp = torch.cat((inp[:,1:], out_char_index.unsqueeze(0)), dim=1)\n",
        "        else:\n",
        "          inp = torch.cat((inp, out_char_index.unsqueeze(0)), dim=1)\n",
        "\n",
        "    return outputs\n",
        "\n",
        "\n",
        "def greedy_sampling_iter_transformer(model, x, num_chars, chunk_len, output_token):\n",
        "    # x -- shape (batch, tokens in x)\n",
        "    outputs = torch.zeros((1,num_chars))\n",
        "    inp = x\n",
        "\n",
        "    for t in range(num_chars):\n",
        "        # b x l x onehot_char\n",
        "        output = model(inp.long())[0,-1:]\n",
        "        output = torch.softmax(output, dim=1)\n",
        "        out_char_index = torch.argmax(output, dim=1)\n",
        "        outputs[:,t] = out_char_index.item()\n",
        "        if inp.shape[1] > chunk_len:\n",
        "          inp = torch.cat((inp[:,1:], out_char_index.unsqueeze(0)), dim=1)\n",
        "        else:\n",
        "          inp = torch.cat((inp, out_char_index.unsqueeze(0)), dim=1)\n",
        "\n",
        "    return outputs\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6X7-Tlc2iqh"
      },
      "source": [
        "## Transformer model training\n",
        "\n",
        "With a correct implementation you should get sensible text generation results with the set parameters, however you should experiment with various parameters,\n",
        "especially with the sequence length (chunk_len) used during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "gY8aZz1R2g3M",
        "outputId": "34ba23d3-e353-4186-c5db-a9ef59f4957c"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "#Sample parameters, use whatever you see fit.\n",
        "batch_size = 256\n",
        "chunk_len = 128\n",
        "train_dataset = TextDataset(chunk_len=chunk_len)\n",
        "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=0)\n",
        "\n",
        "input_dim = train_dataset.n_characters\n",
        "output_dim = train_dataset.n_characters\n",
        "learning_rate = 0.0006\n",
        "\n",
        "model = TransformerSimple(chunk_len, input_dim, output_dim,batch_size)\n",
        "model.train()\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "epochs=50\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    with tqdm(total=len(trainloader.dataset), desc ='Training - Epoch: '+str(epoch)+\"/\"+str(epochs), unit='chunks') as prog_bar:\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            # inputs - shape (batch_size, chunk_len) - Tensor of vocabulary tokens\n",
        "            inputs = data['input'].long()\n",
        "            # labels - shape (batch_size, chunk_len) - Tensor of vocabulary tokens\n",
        "            labels = data['target'].long()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            target_t = labels\n",
        "            loss = criterion(outputs.view(inputs.shape[0]*inputs.shape[1],-1),target_t.view(labels.shape[0]*labels.shape[1]))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            prog_bar.set_postfix(**{'run:': \"Transformer\", 'lr': learning_rate,\n",
        "                                    'loss': loss.item()\n",
        "                                    })\n",
        "            prog_bar.update(batch_size)\n",
        "\n",
        "        # Intermediate text output\n",
        "        sample_texts = [\"What authority surfeits on\",\n",
        "                        \"I say unto you, what he hath done famously, he did it to that end:\",\n",
        "                        \"That in submission will return to us: And then, as we have ta'en the sacrament,\"]\n",
        "        output_token = torch.zeros(1,1).cuda()\n",
        "        output_token[0,0] = train_dataset.n_characters-1\n",
        "        print(\"Top-K sampling\")\n",
        "        for sample_text in sample_texts:\n",
        "            sample_encoding = train_dataset.encode_text(sample_text)\n",
        "            sample_input = Variable(sample_encoding).cuda().unsqueeze(0).long()\n",
        "\n",
        "            #out_test= greedy_sampling_iter_transformer(model, sample_input, 400, chunk_len, output_token)[0]\n",
        "            out_test= topk_sampling_iter_transformer(model, sample_input, 400, chunk_len, output_token)[0]\n",
        "            out_char_index = out_test.long().detach().cpu().numpy()\n",
        "            out_chars = sample_text+\" \"+\"\".join([train_dataset.all_characters[i] for i in out_char_index])\n",
        "            print(\"----------------------------------------\")\n",
        "            print(out_chars)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "or5HqIWQ2qZ3"
      },
      "source": [
        "## Different experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-Tyjm1kHz5Z"
      },
      "source": [
        "## Text sampling - Transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "id": "XjvbjEdjH36Q",
        "outputId": "66b4bc35-0669-488b-df21-a1cf927b1dd2"
      },
      "outputs": [],
      "source": [
        "sample_text = \"Here's to my love! O true apothecary! Thy drugs are quick.\"\n",
        "sample_encoding = train_dataset.encode_text(sample_text)\n",
        "sample_input = Variable(sample_encoding).cuda().unsqueeze(0).long()\n",
        "chunk_len = 128\n",
        "#out_test= greedy_sampling_iter_transformer(model, sample_input, 400, chunk_len, output_token)[0]\n",
        "out_test= topk_sampling_iter_transformer(model, sample_input, 400, chunk_len, output_token)[0]\n",
        "out_char_index = out_test.long().detach().cpu().numpy()\n",
        "out_chars = sample_text+\" \"+\"\".join([train_dataset.all_characters[i] for i in out_char_index])\n",
        "print(\"----------------------------------------\")\n",
        "print(out_chars)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "GvoGzb5T39ia",
        "outputId": "b9023ac3-f353-459b-dfec-3ec8272b05ab"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# List of sequence lengths to experiment with\n",
        "sequence_lengths = [64, 128, 256]\n",
        "# Dictionary to store loss values for each experiment\n",
        "loss_history = {length: [] for length in sequence_lengths}\n",
        "# Dictionary to store generated text samples\n",
        "generated_samples = {length: {} for length in sequence_lengths}\n",
        "\n",
        "# Run each experiment sequentially\n",
        "for chunk_len in sequence_lengths:\n",
        "    print(f\"\\n=== EXPERIMENT WITH CHUNK_LEN = {chunk_len} ===\\n\")\n",
        "\n",
        "    # Initialize dataset, model and training components\n",
        "    batch_size = 256\n",
        "    train_dataset = TextDataset(chunk_len=chunk_len)\n",
        "    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=0)\n",
        "    input_dim = train_dataset.n_characters\n",
        "    output_dim = train_dataset.n_characters\n",
        "    learning_rate = 0.0006\n",
        "\n",
        "    # Create a new model for this experiment\n",
        "    model = TransformerSimple(chunk_len, input_dim, output_dim, batch_size)\n",
        "    model.train()\n",
        "    model.cuda()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    epochs = 50\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0.0\n",
        "        with tqdm(total=len(trainloader.dataset), desc=f'Training chunk_len={chunk_len} - Epoch: {epoch}/{epochs}', unit='chunks') as prog_bar:\n",
        "            for i, data in enumerate(trainloader, 0):\n",
        "                # Training step\n",
        "                inputs = data['input'].long()\n",
        "                labels = data['target'].long()\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                target_t = labels\n",
        "                loss = criterion(outputs.view(inputs.shape[0]*inputs.shape[1],-1), target_t.view(labels.shape[0]*labels.shape[1]))\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                # Update progress bar\n",
        "                epoch_loss = loss.item()\n",
        "                prog_bar.set_postfix(**{'run:': f\"Transformer_{chunk_len}\", 'lr': learning_rate, 'loss': epoch_loss})\n",
        "                prog_bar.update(batch_size)\n",
        "\n",
        "        # Store the loss for this epoch\n",
        "        loss_history[chunk_len].append(epoch_loss)\n",
        "\n",
        "        # Generate samples every 10 epochs to track progress\n",
        "        if epoch % 10 == 9 or epoch == epochs - 1:\n",
        "            sample_texts = [\n",
        "                \"What authority surfeits on\",\n",
        "                \"I say unto you, what he hath done famously, he did it to that end:\",\n",
        "                \"That in submission will return to us: And then, as we have ta'en the sacrament,\"\n",
        "            ]\n",
        "            output_token = torch.zeros(1, 1).cuda()\n",
        "            output_token[0, 0] = train_dataset.n_characters - 1\n",
        "\n",
        "            print(f\"\\nTop-K sampling - chunk_len {chunk_len} - Epoch {epoch}\")\n",
        "            samples_for_epoch = {}\n",
        "\n",
        "            for sample_text in sample_texts:\n",
        "                sample_encoding = train_dataset.encode_text(sample_text)\n",
        "                sample_input = Variable(sample_encoding).cuda().unsqueeze(0).long()\n",
        "                out_test = topk_sampling_iter_transformer(model, sample_input, 400, chunk_len, output_token)[0]\n",
        "                out_char_index = out_test.long().detach().cpu().numpy()\n",
        "                out_chars = sample_text + \" \" + \"\".join([train_dataset.all_characters[i] for i in out_char_index])\n",
        "\n",
        "                print(\"----------------------------------------\")\n",
        "                print(out_chars)\n",
        "\n",
        "                # Store the generated text\n",
        "                samples_for_epoch[sample_text] = out_chars\n",
        "\n",
        "            # Add samples for this epoch to the dictionary\n",
        "            generated_samples[chunk_len][epoch] = samples_for_epoch\n",
        "\n",
        "    # Save the model for this experiment if needed\n",
        "    torch.save(model.state_dict(), f'transformer_model_chunk_len_{chunk_len}.pt')\n",
        "\n",
        "# Plot loss curves for comparison\n",
        "plt.figure(figsize=(10, 6))\n",
        "for chunk_len, losses in loss_history.items():\n",
        "    plt.plot(range(1, epochs + 1), losses, label=f'chunk_len={chunk_len}')\n",
        "\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss by Sequence Length')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig('loss_comparison.png')\n",
        "plt.show()\n",
        "\n",
        "# Print summary of results\n",
        "print(\"\\n=== SUMMARY OF RESULTS ===\\n\")\n",
        "for chunk_len in sequence_lengths:\n",
        "    final_loss = loss_history[chunk_len][-1]\n",
        "    print(f\"Sequence Length {chunk_len}: Final Loss = {final_loss:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
